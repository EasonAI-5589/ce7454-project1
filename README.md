# CE7454 Face Parsing - MicroSegFormer

Face parsing project using MicroSegFormer (1.72M parameters).

**Deadline**: October 14, 2024 11:59 PM
**Status**: Ready for Training âœ…

## ğŸ¯ Project Overview

- **Task**: Face parsing with 19-class semantic segmentation
- **Dataset**: CelebAMask-HQ mini (1000 train + 100 val images)
- **Model**: MicroSegFormer - Hierarchical Transformer
- **Parameters**: 1,723,027 (94.6% of 1,821,085 limit)
- **Target**: F-Score > 0.80

## ğŸš€ Quick Start (One Command)

```bash
# Start training immediately
./quick_start.sh train
```

That's it! Training will begin with optimal settings.

## ğŸ“ Setup

### 1. Clone & Install
```bash
git clone https://github.com/EasonAI-5589/ce7454-project1.git
cd ce7454-project1
pip install -r requirements.txt
```

### 2. Dataset Setup

Download from [CodaBench](https://www.codabench.org/competitions/4381/):
```bash
# Extract dataset
unzip dev-public.zip

# Verify structure
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/  # 1000 images
â”‚   â””â”€â”€ masks/   # 1000 masks
```

### 3. Train Model
```bash
# Start training
./quick_start.sh train

# Or with Python
python main.py --config configs/main.yaml
```

## ğŸ“Š Model Architecture

**MicroSegFormer** - Hierarchical Transformer for Face Parsing

```
Input (512Ã—512Ã—3)
    â†“
Hierarchical Encoder (4 stages)
â”œâ”€â”€ Stage 1: 64 channels  â†’ 128Ã—128
â”œâ”€â”€ Stage 2: 128 channels â†’ 64Ã—64
â”œâ”€â”€ Stage 3: 256 channels â†’ 32Ã—32
â””â”€â”€ Stage 4: 512 channels â†’ 16Ã—16
    â†“
Multi-Scale Fusion
â”œâ”€â”€ Skip connections
â””â”€â”€ Self-attention modules
    â†“
Lightweight MLP Decoder
    â†“
Output (512Ã—512Ã—19)
```

**Key Features**:
- 1,723,027 parameters (94.6% usage)
- Hierarchical transformer encoder
- Multi-scale feature fusion
- Efficient attention mechanism
- Optimized for face parsing

## ğŸ¯ Training Configuration

**Default settings** (configs/main.yaml):
- **Epochs**: 150 with early stopping (patience=20)
- **Batch Size**: 8
- **Optimizer**: AdamW (lr=1e-3, wd=1e-4)
- **Scheduler**: CosineAnnealingLR with 5-epoch warmup
- **Loss**: CrossEntropy (1.0) + Dice (0.5)
- **Augmentation**: Flip, rotation, color jitter, scaling

**Expected Training Time**: 4-6 hours on V100/A100

## ğŸ“ Commands Reference

### Training
```bash
# Start training
./quick_start.sh train

# Resume from checkpoint
./quick_start.sh resume checkpoints/best_model.pth

# Specify device
python main.py --device cuda:0
```

### Testing
```bash
# Test model on validation set
./quick_start.sh test checkpoints/best_model.pth

# Or with Python
python test.py --checkpoint checkpoints/best_model.pth
```

### Monitor Training
```bash
# Watch training logs
tail -f checkpoints/microsegformer_*/training_log.txt

# Check model parameters
python main.py --config configs/main.yaml  # Prints param count
```

## ğŸ“‚ Project Structure

```
ce7454-project1/
â”œâ”€â”€ main.py                  # Main training entry
â”œâ”€â”€ test.py                  # Model evaluation
â”œâ”€â”€ quick_start.sh           # One-command training
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ main.yaml           # Model configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ trainer.py          # Training loop
â”‚   â”œâ”€â”€ dataset.py          # Data loading
â”‚   â”œâ”€â”€ augmentation.py     # Data augmentation
â”‚   â”œâ”€â”€ inference.py        # Inference utilities
â”‚   â”œâ”€â”€ utils.py            # Helper functions
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ microsegformer.py  # Model architecture
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md
â”‚   â”œâ”€â”€ TRAINING_GUIDE.md
â”‚   â””â”€â”€ CODABENCH_README.md
â””â”€â”€ data/                   # Dataset (not in git)
    â””â”€â”€ train/
        â”œâ”€â”€ images/
        â””â”€â”€ masks/
```

## ğŸ“ Training Outputs

After training, you'll get:

```
checkpoints/microsegformer_YYYYMMDD_HHMMSS/
â”œâ”€â”€ best_model.pth          # Best F-Score checkpoint â­
â”œâ”€â”€ last_model.pth          # Latest checkpoint
â”œâ”€â”€ config.yaml             # Training config backup
â””â”€â”€ training_log.txt        # Training metrics
```

**Checkpoint contains**:
- Model weights
- Optimizer state
- Scheduler state
- Best F-Score
- Training epoch

## ğŸ“Š Performance Targets

| Metric | Target | Status |
|--------|--------|--------|
| **F-Score** | > 0.80 | ğŸ¯ Goal |
| **Parameters** | 1,723,027 | âœ… Under limit |
| **Training Time** | 4-6 hours | âœ… Optimized |
| **GPU Memory** | ~6GB | âœ… Efficient |

## ğŸ”§ Troubleshooting

**CUDA Out of Memory**
```yaml
# In configs/main.yaml, reduce batch size
batch_size: 4  # or 2
```

**Training too slow**
```yaml
# Reduce workers if CPU bottleneck
num_workers: 2
```

**Low F-Score**
- Train longer (increase epochs to 200)
- Check data augmentation settings
- Verify data quality

## ğŸ“š Documentation

Detailed guides in `docs/`:
- **[PROJECT_OVERVIEW.md](docs/PROJECT_OVERVIEW.md)** - Project structure, model info
- **[TRAINING_GUIDE.md](docs/TRAINING_GUIDE.md)** - Complete training guide
- **[CODABENCH_README.md](docs/CODABENCH_README.md)** - Submission instructions

## ğŸ¯ Next Steps

1. âœ… Setup environment
2. âœ… Download dataset
3. **Run training**: `./quick_start.sh train`
4. **Monitor progress**: Check training logs
5. **Test model**: Use best checkpoint
6. **Submit to CodaBench**

## ğŸ“‹ Requirements

- **Python**: 3.8+
- **PyTorch**: 2.0+
- **GPU**: 6GB+ VRAM recommended
- **Disk**: 2GB for dataset + checkpoints

See [requirements.txt](requirements.txt) for dependencies.

## ğŸ† Competition Rules

- âœ… Single model (no ensemble)
- âœ… No pretrained weights
- âœ… No external data
- âœ… < 1,821,085 parameters

## ğŸ“– References

- [CelebAMask-HQ Dataset](https://github.com/switchablenorms/CelebAMask-HQ)
- [Competition Page](https://www.codabench.org/competitions/4381/)
- [SegFormer Paper](https://arxiv.org/abs/2105.15203)

---

**CE7454 Deep Learning for Data Science | NTU 2024**
