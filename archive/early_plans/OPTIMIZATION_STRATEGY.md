# æ¨¡åž‹ä¼˜åŒ–ç­–ç•¥ - ç›®æ ‡æå‡F-Scoreè‡³0.75+

## å½“å‰çŠ¶æ€åˆ†æž

### æ€§èƒ½æŒ‡æ ‡
- **å½“å‰æœ€ä½³ Val F-Score**: 0.648 (Epoch 98)
- **ç›®æ ‡**: 0.75+ (éœ€è¦æå‡ **+16%**)
- **è¿‡æ‹Ÿåˆç¨‹åº¦**: 11.3% (Train 0.704 vs Val 0.632)

### å‘çŽ°çš„é—®é¢˜

#### ðŸš¨ é—®é¢˜1: ä¸¥é‡è¿‡æ‹Ÿåˆ (11.3% gap)
**åŽŸå› **:
- æ¨¡åž‹å®Œå…¨æ²¡æœ‰dropout
- Weight decayå¯èƒ½ä¸è¶³ (å½“å‰5e-4)
- æ•°æ®å¢žå¼ºè™½å¼ºä½†æ¨¡åž‹å®¹é‡å¤ªå¤§

**å·²å®žæ–½çš„è§£å†³æ–¹æ¡ˆ**:
- âœ… æ·»åŠ 0.15 progressive dropoutåˆ°æ‰€æœ‰å±‚
- âœ… é™ä½Žweight decayåˆ°1e-4
- âœ… é™ä½Žå­¦ä¹ çŽ‡ 1.5e-3 â†’ 8e-4

#### ðŸš¨ é—®é¢˜2: F-Scoreæ•´ä½“åä½Ž (0.648)
**å¯èƒ½åŽŸå› **:
- ç±»åˆ«ä¸å¹³è¡¡ä¸¥é‡ (Class 16æœ‰16.3å€å·®å¼‚)
- æ²¡æœ‰class weights
- æŸå¤±å‡½æ•°å¯èƒ½ä¸å¤Ÿä¼˜åŒ–
- è®­ç»ƒç­–ç•¥ä¸å¤Ÿæ¿€è¿›

#### ðŸš¨ é—®é¢˜3: è®­ç»ƒä»åœ¨æ”¹å–„ (æœ€åŽ50è½® +0.02)
**è¯´æ˜Ž**:
- 150 epochsä¸å¤Ÿï¼Œæ¨¡åž‹æœªå®Œå…¨æ”¶æ•›
- å­¦ä¹ çŽ‡ä¸‹é™å¤ªå¿« (æœ€ç»ˆåªæœ‰1.6e-4)

---

## ðŸŽ¯ ä¼˜åŒ–ç­–ç•¥ (æŒ‰ä¼˜å…ˆçº§)

### ä¼˜å…ˆçº§1: å‡å°‘è¿‡æ‹Ÿåˆ â­â­â­â­â­

#### 1.1 ä½¿ç”¨dropoutç‰ˆæœ¬æ¨¡åž‹ (å·²å®Œæˆ)
```yaml
model:
  dropout: 0.15  # å·²æ·»åŠ 
```
**é¢„æœŸæå‡**: Val F-Score +2-3% (å‡å°‘è¿‡æ‹Ÿåˆ)

#### 1.2 å¢žåŠ è®­ç»ƒè½®æ•°
```yaml
training:
  epochs: 200  # ä»Ž150å¢žåŠ 
  early_stopping_patience: 50  # ä»Ž30å¢žåŠ 
```
**åŽŸå› **: å½“å‰è®­ç»ƒè¶‹åŠ¿ä»åœ¨ä¸Šå‡
**é¢„æœŸæå‡**: +1-2%

#### 1.3 è°ƒæ•´å­¦ä¹ çŽ‡ç­–ç•¥
**å½“å‰é—®é¢˜**: LRä¸‹é™å¤ªå¿« (1.5e-3 â†’ 1.6e-4 in 150 epochs)

**æ–¹æ¡ˆA**: æé«˜åˆå§‹LR + Warmup
```yaml
training:
  learning_rate: 1e-3  # ä»Ž1.5e-3é™ä½Ž
  warmup_epochs: 10    # ä»Ž5å¢žåŠ 
  scheduler: CosineAnnealingWarmRestarts  # å‘¨æœŸæ€§é‡å¯
```

**æ–¹æ¡ˆB**: ä¿å®ˆç­–ç•¥ (æŽ¨è)
```yaml
training:
  learning_rate: 8e-4  # æ›´ä¿å®ˆ
  warmup_epochs: 5
  scheduler: CosineAnnealingLR
  T_max: 180  # 180 epochsåŽåˆ°è¾¾æœ€å°å€¼
```

---

### ä¼˜å…ˆçº§2: å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ â­â­â­â­

#### 2.1 æ·»åŠ Class Weights
**åˆ†æž**: Class 16å‡ ä¹Žä¸å­˜åœ¨ (16.3xä¸å¹³è¡¡)

**å®žæ–½æ­¥éª¤**:
1. è®¡ç®—ç±»åˆ«æƒé‡
```python
# In src/utils.py
def compute_class_weights(data_dir, num_classes=19):
    from glob import glob
    class_counts = np.zeros(num_classes)

    for mask_file in glob(f"{data_dir}/masks/*.png"):
        mask = np.array(Image.open(mask_file))
        for cls in range(num_classes):
            class_counts[cls] += (mask == cls).sum()

    # Inverse frequency weighting
    total = class_counts.sum()
    weights = total / (num_classes * (class_counts + 1.0))
    weights = weights / weights.mean()  # Normalize

    # Cap extreme weights
    weights = np.clip(weights, 0.1, 10.0)
    return torch.FloatTensor(weights)
```

2. åœ¨CombinedLossä¸­ä½¿ç”¨
```python
class CombinedLoss(nn.Module):
    def __init__(self, ce_weight=1.0, dice_weight=0.5, class_weights=None):
        super().__init__()
        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)
        ...
```

**é¢„æœŸæå‡**: +3-5% (æ˜¾è‘—æ”¹å–„å°ç±»åˆ«F-Score)

#### 2.2 Focal Loss (å¯é€‰)
å¯¹äºŽæžåº¦ä¸å¹³è¡¡çš„ç±»åˆ«ï¼Œè€ƒè™‘Focal Loss:
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()
```

**é¢„æœŸæå‡**: +1-2%

---

### ä¼˜å…ˆçº§3: ä¼˜åŒ–æ•°æ®å’Œè®­ç»ƒ â­â­â­

#### 3.1 å¤šå°ºåº¦è®­ç»ƒ
**å½“å‰**: å›ºå®š512x512
**æ”¹è¿›**: éšæœºå°ºåº¦ [384, 448, 512, 576]

```python
# In dataset.py
class MultiScaleResize:
    def __init__(self, scales=[384, 448, 512, 576]):
        self.scales = scales

    def __call__(self, img, mask):
        scale = random.choice(self.scales)
        img = cv2.resize(img, (scale, scale))
        mask = cv2.resize(mask, (scale, scale), interpolation=cv2.INTER_NEAREST)

        # Crop to 512x512
        if scale > 512:
            x = random.randint(0, scale - 512)
            y = random.randint(0, scale - 512)
            img = img[y:y+512, x:x+512]
            mask = mask[y:y+512, x:x+512]
        elif scale < 512:
            # Pad to 512
            ...
        return img, mask
```

**é¢„æœŸæå‡**: +1-2% (æ›´å¥½çš„æ³›åŒ–)

#### 3.2 Mixup/CutMix (è°¨æ…Žä½¿ç”¨)
åˆ†å‰²ä»»åŠ¡ä¸­æ•ˆæžœä¸ç¡®å®šï¼Œä½†å¯ä»¥å°è¯•ï¼š
```python
def cutmix(img1, mask1, img2, mask2, alpha=0.5):
    lam = np.random.beta(alpha, alpha)
    H, W = img1.shape[:2]
    cut_w = int(W * np.sqrt(1 - lam))
    cut_h = int(H * np.sqrt(1 - lam))

    cx = np.random.randint(W)
    cy = np.random.randint(H)

    x1 = np.clip(cx - cut_w // 2, 0, W)
    x2 = np.clip(cx + cut_w // 2, 0, W)
    y1 = np.clip(cy - cut_h // 2, 0, H)
    y2 = np.clip(cy + cut_h // 2, 0, H)

    img1[y1:y2, x1:x2] = img2[y1:y2, x1:x2]
    mask1[y1:y2, x1:x2] = mask2[y1:y2, x1:x2]

    return img1, mask1
```

**é¢„æœŸæå‡**: +0.5-1% (å¯èƒ½è´Ÿé¢)

#### 3.3 åœ¨çº¿éš¾æ ·æœ¬æŒ–æŽ˜
æ¯ä¸ªepochåŽåˆ†æžé¢„æµ‹é”™è¯¯æœ€ä¸¥é‡çš„æ ·æœ¬ï¼Œå¢žåŠ å…¶é‡‡æ ·æƒé‡ï¼š
```python
class WeightedRandomSampler:
    # æ ¹æ®ä¸Šä¸€è½®çš„F-Scoreç»™æ ·æœ¬åŠ æƒ
    # ä½ŽF-Scoreæ ·æœ¬æƒé‡é«˜
```

**é¢„æœŸæå‡**: +1-2%

---

### ä¼˜å…ˆçº§4: æ¨¡åž‹æž¶æž„ä¼˜åŒ– â­â­

#### 4.1 å¢žåŠ æ¨¡åž‹æ·±åº¦ (åœ¨å‚æ•°é™åˆ¶å†…)
**å½“å‰**: depths=[1, 2, 2, 2] (7å±‚)
**å¯èƒ½**: depths=[2, 2, 3, 2] (9å±‚)

éœ€è¦å‡å°embed_dimsä¿æŒå‚æ•° <1.82M:
```python
embed_dims = [28, 56, 112, 180]  # ä»Ž[32, 64, 128, 192]å‡å°
depths = [2, 2, 3, 2]
```

**é¢„æœŸæå‡**: +1-2% (æ›´æ·±çš„ç‰¹å¾)

#### 4.2 æ·»åŠ è¾…åŠ©æŸå¤±
åœ¨decoderä¸­é—´å±‚æ·»åŠ è¾…åŠ©ç›‘ç£ï¼š
```python
class MLPDecoder:
    def forward(self, ...):
        # ...
        # Add auxiliary head at stage 3
        aux_out = self.aux_head(_c3)  # Lightweight prediction
        return main_out, aux_out

# Loss
loss = main_loss + 0.4 * aux_loss
```

**é¢„æœŸæå‡**: +1-2%

---

## ðŸ“‹ æŽ¨èçš„å®žæ–½è®¡åˆ’

### é˜¶æ®µ1: ç«‹å³å¯åš (1-2å°æ—¶)

1. âœ… **æ·»åŠ dropout** (å·²å®Œæˆ)
2. âš¡ **è®¡ç®—å¹¶æ·»åŠ class weights** (ä»£ç å·²ç»™å‡º)
   - é¢„æœŸæ•ˆæžœæœ€æ˜Žæ˜¾ (+3-5%)
3. âš¡ **è°ƒæ•´è®­ç»ƒé…ç½®**:
   - epochs: 200
   - learning_rate: 8e-4
   - early_stopping_patience: 50

**é¢„æœŸæ€»æå‡**: Val F-Score 0.648 â†’ **0.70-0.72**

### é˜¶æ®µ2: å¦‚æžœé˜¶æ®µ1æ•ˆæžœä¸å¤Ÿ (2-3å°æ—¶)

4. å®žæ–½å¤šå°ºåº¦è®­ç»ƒ
5. æ·»åŠ Focal Lossæˆ–è°ƒæ•´class weights
6. åœ¨çº¿éš¾æ ·æœ¬æŒ–æŽ˜

**é¢„æœŸæ€»æå‡**: Val F-Score 0.70-0.72 â†’ **0.73-0.76**

### é˜¶æ®µ3: æœ€åŽå†²åˆº (éœ€è¦é‡æ–°è®¾è®¡)

7. å°è¯•æ›´æ·±çš„æž¶æž„
8. æ·»åŠ è¾…åŠ©æŸå¤±
9. é›†æˆå­¦ä¹  (è®­ç»ƒ3ä¸ªæ¨¡åž‹ï¼Œæµ‹è¯•æ—¶ensemble)

**é¢„æœŸæ€»æå‡**: Val F-Score 0.73-0.76 â†’ **0.76-0.80**

---

## ðŸš€ å¿«é€Ÿå¼€å§‹ä»£ç 

### æ·»åŠ Class Weights

```python
# 1. è®¡ç®—æƒé‡ (è¿è¡Œä¸€æ¬¡)
python -c "
from src.utils import compute_class_weights
weights = compute_class_weights('data/train')
print('Class weights:', weights.tolist())
import torch
torch.save(weights, 'data/class_weights.pt')
"

# 2. åœ¨trainer.pyä¸­åŠ è½½
class_weights = torch.load('data/class_weights.pt').to(device)
criterion = CombinedLoss(ce_weight=1.0, dice_weight=0.5, class_weights=class_weights)

# 3. è®­ç»ƒ
python main.py --config configs/optimized.yaml
```

---

## ðŸ“Š é¢„æœŸæœ€ç»ˆç»“æžœ

| é˜¶æ®µ | æ”¹è¿› | Val F-Score | è¾¾æˆç›®æ ‡ |
|------|------|-------------|----------|
| å½“å‰ | - | 0.648 | âŒ |
| +Dropout | å‡å°‘è¿‡æ‹Ÿåˆ | 0.66-0.68 | âŒ |
| +Class Weights | å¤„ç†ä¸å¹³è¡¡ | 0.70-0.72 | âš ï¸ |
| +å¤šå°ºåº¦è®­ç»ƒ | æ›´å¥½æ³›åŒ– | 0.73-0.76 | âœ… |
| +æž¶æž„ä¼˜åŒ– | æå‡å®¹é‡ | 0.76-0.80 | âœ…âœ… |

**ä¿å®ˆä¼°è®¡**: 0.72-0.75 (æ»¡è¶³ç«žèµ›è¦æ±‚)
**ç†æƒ³æƒ…å†µ**: 0.76-0.78 (å‰25%)
**æœ€ä½³æƒ…å†µ**: 0.80+ (å‰10%)

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å‚æ•°é™åˆ¶**: å§‹ç»ˆä¿æŒ <1,821,085 å‚æ•°
2. **æäº¤å…¼å®¹æ€§**: ç¡®ä¿submission/solution/ä¸Žè®­ç»ƒä»£ç ä¸€è‡´
3. **éªŒè¯é›†å¤§å°**: å½“å‰åªæœ‰10% (100å¼ )ï¼Œç»“æžœå¯èƒ½æœ‰æ³¢åŠ¨
4. **æ—¶é—´é™åˆ¶**: 10æœˆ14æ—¥æˆªæ­¢ï¼Œä¼˜å…ˆå®žæ–½é«˜ä¼˜å…ˆçº§æ”¹è¿›

**å»ºè®®**: å…ˆå®žæ–½é˜¶æ®µ1ï¼Œè®­ç»ƒçœ‹æ•ˆæžœã€‚å¦‚æžœè¾¾åˆ°0.70+ï¼Œå†è€ƒè™‘é˜¶æ®µ2ã€‚
