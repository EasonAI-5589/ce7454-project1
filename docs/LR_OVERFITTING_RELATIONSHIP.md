# å­¦ä¹ ç‡é™ä½å¯¼è‡´è¿‡æ‹Ÿåˆ - æ·±åº¦åˆ†æ

**å…³é”®å‘ç°**: å­¦ä¹ ç‡è¶Šä½ï¼Œæ¨¡å‹è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼

---

## ğŸ“Š å®éªŒè¯æ®

### æ•°æ®åˆ†æ (Best Model 0.6889)

| é˜¶æ®µ | Epoch | å¹³å‡LR | Train F-Score | Val F-Score | Train-Val Gap |
|------|-------|--------|---------------|-------------|---------------|
| **Early** | 1-30 | 0.000748 | 0.3932 | 0.3998 | **-0.0066** (-1.6%) æ¬ æ‹Ÿåˆ |
| **Mid** | 31-60 | 0.000762 | 0.6183 | 0.6203 | **-0.0020** (-0.3%) å‡ ä¹å®Œç¾ |
| **Late** | 61-92 | 0.000688 | 0.6777 | 0.6519 | **+0.0258** (+4.0%) è½»å¾®è¿‡æ‹Ÿåˆ |
| **Overfit** | 93-112 | 0.000603 | 0.7107 | 0.6503 | **+0.0604** (+9.3%) ä¸¥é‡è¿‡æ‹Ÿåˆ |

### å…³é”®è¶‹åŠ¿

```
LRé™ä½:     0.000748 â†’ 0.000603 (-19.4%)
Gapå¢åŠ :    -0.0066  â†’ +0.0604  (+1018%!!!)

æ˜æ˜¾ç›¸å…³æ€§: LRè¶Šä½ â†’ Gapè¶Šå¤§ â†’ è¿‡æ‹Ÿåˆè¶Šä¸¥é‡
```

---

## ğŸ¤” ä¸ºä»€ä¹ˆLRè¶Šä½è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Ÿ

### ç†è®º1: å‚æ•°æ›´æ–°å¹…åº¦ä¸æ³›åŒ–èƒ½åŠ›

**é«˜å­¦ä¹ ç‡é˜¶æ®µ (LR = 0.0007-0.0008)**:
```python
# å‚æ•°æ›´æ–°å¹…åº¦å¤§
weight_update = lr * gradient = 0.0008 * grad
â†’ å‚æ•°æ¯æ­¥å˜åŒ–è¾ƒå¤§
â†’ æ¨¡å‹è¢«è¿«å­¦ä¹ "é²æ£’"çš„ç‰¹å¾ï¼ˆæ³›åŒ–å¥½ï¼‰
â†’ æ— æ³•ç²¾ç»†æ‹Ÿåˆè®­ç»ƒé›†å™ªå£°
```

**ä½å­¦ä¹ ç‡é˜¶æ®µ (LR = 0.0006ä»¥ä¸‹)**:
```python
# å‚æ•°æ›´æ–°å¹…åº¦å°
weight_update = lr * gradient = 0.0006 * grad
â†’ å‚æ•°æ¯æ­¥å˜åŒ–å¾ˆå°
â†’ æ¨¡å‹å¯ä»¥ç²¾ç»†è°ƒæ•´ï¼Œæ‹Ÿåˆè®­ç»ƒé›†ç»†èŠ‚
â†’ å¼€å§‹è®°å¿†è®­ç»ƒé›†å™ªå£° â†’ è¿‡æ‹Ÿåˆï¼
```

---

### ç†è®º2: ä¼˜åŒ–åŠ¨åŠ›å­¦è§†è§’

**Learning Rateçš„åŒé‡ä½œç”¨**:

1. **Exploration (æ¢ç´¢)**: é«˜LR â†’ è·³å‡ºå±€éƒ¨æœ€ä¼˜ â†’ å¯»æ‰¾æ³›åŒ–è§£
2. **Exploitation (åˆ©ç”¨)**: ä½LR â†’ ç²¾ç»†ä¼˜åŒ– â†’ å¯èƒ½è¿‡æ‹Ÿåˆ

**å®éªŒè§‚å¯Ÿ**:
```
Epoch 1-60:  LR=0.0007-0.0008  â†’ å¿«é€Ÿæå‡ï¼Œæ³›åŒ–å¥½ (Gap<0.01)
Epoch 61-92: LR=0.0006-0.0007  â†’ ç»§ç»­ä¼˜åŒ–ï¼Œå¼€å§‹è¿‡æ‹Ÿåˆ (Gap=0.026)
Epoch 93+:   LR<0.0006         â†’ æ— æå‡ï¼Œä¸¥é‡è¿‡æ‹Ÿåˆ (Gap=0.060)
```

**ä¸´ç•Œç‚¹**: **LR < 0.0006** æ—¶ï¼Œæ¨¡å‹ä»"å­¦ä¹ "è½¬ä¸º"è®°å¿†"

---

### ç†è®º3: Sharpness vs Flatness

**é«˜LRæ‰¾åˆ°Flat Minima (æ³›åŒ–å¥½)**:
```
Loss Landscape (é«˜LR):
    â•±â•²        â† æ— æ³•è¿›å…¥sharp minima
   â•±  â•²
  â•±    â•²      â† åªèƒ½åœç•™åœ¨flatåŒºåŸŸ (æ³›åŒ–å¥½)
 â•±      â•²
â•â•â•â•â•â•â•â•â•â•â•â•
```

**ä½LRé™·å…¥Sharp Minima (è¿‡æ‹Ÿåˆ)**:
```
Loss Landscape (ä½LR):
      â•±â•²        â† å¯ä»¥ç²¾ç»†è¿›å…¥sharp minima
     â•±  â•²
    â•±    â•²      â† è®­ç»ƒlossä½ï¼Œä½†æ³›åŒ–å·®
   â•±      â•²
â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“ˆ å¯è§†åŒ–åˆ†æ

### LR vs Train-Val Gap

```
Train-Val Gap
0.07 â”¤                                        â—â—
0.06 â”¤                                    â—â—â—â—
0.05 â”¤                                â—â—â—â—
0.04 â”¤                            â—â—â—â—
0.03 â”¤                        â—â—â—â—
0.02 â”¤                    â—â—â—â—
0.01 â”¤                â—â—â—â—
0.00 â”¤            â—â—â—â—
-0.01â”¤    â—â—â—â—â—â—â—â—
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     0.0008  0.0007  0.0006  0.0005  Learning Rate

æ¸…æ™°è¶‹åŠ¿: LRé™ä½ â†’ Gapå¢å¤§ â†’ è¿‡æ‹ŸåˆåŠ å‰§
```

### Epoch vs LR vs Gap

```
Metric
1.0  â”¤ Train F-Score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0.8  â”¤           â•±â•±â•±â•±â•±â•±â•±â•±â•±â•±â•±â”€â”€â”€â”€â”€â”€â”€â”€
0.7  â”¤       â•±â•±â•±â•±              Val F-Score
0.6  â”¤   â•±â•±â•±â•±                    â•²â•²â•²
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0.08 â”¤                              â•±â•±â•±â•±
0.06 â”¤                          â•±â•±â•±â•±
0.04 â”¤                      â•±â•±â•±â•±  Gap
0.02 â”¤                  â•±â•±â•±â•±
0.00 â”¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â•±â•±â•±
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LR   â”¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²
     0    30    60    92    112  Epoch
              â†‘
         Gapå¼€å§‹å¢å¤§ï¼ŒLRå¼€å§‹é™ä½
```

---

## ğŸ¯ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: Warm Restarts â­â­â­â­â­ (æ¨è)

**åŸç†**: å‘¨æœŸæ€§é‡å¯LRï¼Œé¿å…åœ¨ä½LRé˜¶æ®µè¿‡æ‹Ÿåˆ

```yaml
scheduler: CosineAnnealingWarmRestarts
scheduler_params:
  T_0: 25
  T_mult: 2
  eta_min: 1e-6
```

**LRæ›²çº¿**:
```
LR
0.0008 â”¤â”€â”€â•®   â•­â”€â”€â•®        â•­â”€â”€â•®
0.0007 â”¤   â•² â•±   â•²      â•±   â•²
0.0006 â”¤    â•³     â•²    â•±     â•²
       â”¤   â•± â•²     â•²  â•±       â•²
       â”¤  â•±   â•²     â•²â•±         â•²
       0   25   50    100     200  Epoch
           â†‘é‡å¯  â†‘é‡å¯

å¥½å¤„:
1. å‘¨æœŸæ€§å›åˆ°é«˜LR â†’ é‡æ–°æ¢ç´¢ â†’ é¿å…è¿‡æ‹Ÿåˆ
2. å¤šæ¬¡æ”¶æ•›æœºä¼š â†’ å¯èƒ½æ‰¾åˆ°æ›´å¥½çš„è§£
3. æ¯æ¬¡ä½LRé˜¶æ®µè¾ƒçŸ­ â†’ è¿‡æ‹Ÿåˆæ—¶é—´çŸ­
```

**é¢„æœŸæ•ˆæœ**:
- å‡å°‘è¿‡æ‹Ÿåˆ (Gapä»9.3%é™åˆ°5%ä»¥ä¸‹)
- æå‡Val F-Score (+0.5-1.0%)

---

### æ–¹æ¡ˆ2: æ›´å¼ºæ­£åˆ™åŒ– â­â­â­â­

**åŸç†**: åœ¨ä½LRé˜¶æ®µå¢åŠ æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è®°å¿†è®­ç»ƒé›†

```yaml
model:
  dropout: 0.2  # ä»0.15æå‡

training:
  weight_decay: 2e-4  # ä»1e-4æå‡

loss:
  label_smoothing: 0.1  # æ–°å¢
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**:
- **Dropout 0.2**: å¼ºåˆ¶æ¨¡å‹ä¸ä¾èµ–ç‰¹å®šç¥ç»å…ƒ â†’ å‡å°‘è®°å¿†
- **Weight Decay 2e-4**: L2æ­£åˆ™åŒ– â†’ å‚æ•°ä¸ä¼šè¿‡åº¦æ‹Ÿåˆç»†èŠ‚
- **Label Smoothing**: è½¯åŒ–æ ‡ç­¾ â†’ æ¨¡å‹ä¸ä¼šè¿‡åº¦è‡ªä¿¡

**é¢„æœŸ**: Gapä»9.3%é™åˆ°6-7%

---

### æ–¹æ¡ˆ3: Early Stoppingä¼˜åŒ– â­â­â­

**å½“å‰é—®é¢˜**:
```
Patience = 30
Best@92, Stop@112 (30è½®å)
ä½†92-112è½®éƒ½åœ¨è¿‡æ‹Ÿåˆï¼
```

**æ”¹è¿›ç­–ç•¥1: ç›‘æ§Gapè€Œä¸åªæ˜¯Val**:
```python
# ä¼ªä»£ç 
if val_f_score > best_val:
    best_val = val_f_score
    patience_counter = 0
elif train_val_gap > 0.05:  # æ–°å¢æ¡ä»¶
    print("Gapè¿‡å¤§ï¼Œæå‰åœæ­¢")
    break
```

**æ”¹è¿›ç­–ç•¥2: Patience=100ä½†é…åˆWarm Restarts**:
```yaml
early_stopping_patience: 100
# ä½†æœ‰Warm Restartsï¼Œä¸ä¼šä¸€ç›´ä½LR
```

---

### æ–¹æ¡ˆ4: åŠ¨æ€å­¦ä¹ ç‡ä¸‹é™ â­â­â­

**åŸç†**: ä¸è®©LRé™å¤ªä½

```yaml
scheduler: CosineAnnealingLR
scheduler_params:
  T_max: 150
  eta_min: 0.0002  # ğŸ”¥ æé«˜æœ€ä½LR (åŸæ¥æ˜¯1e-6)
```

**æ•ˆæœ**:
```
åŸé…ç½®: LRä»0.0008é™åˆ°0.00057 (å¤ªä½!)
æ–°é…ç½®: LRä»0.0008é™åˆ°0.0002  (ä¿æŒä¼˜åŒ–èƒ½åŠ›)
```

---

## ğŸ’¡ æœ€ä½³ç»„åˆæ–¹æ¡ˆ

### ç»ˆæé…ç½® (è§£å†³æ‰€æœ‰é—®é¢˜)

```yaml
loss:
  ce_weight: 1.0
  dice_weight: 1.5  # å·²éªŒè¯æœ€ä¼˜
  label_smoothing: 0.1  # é˜²æ­¢è¿‡åº¦è‡ªä¿¡

model:
  dropout: 0.2  # å¼ºæ­£åˆ™åŒ–

training:
  learning_rate: 8e-4
  weight_decay: 2e-4  # æ›´å¼ºL2

  scheduler: CosineAnnealingWarmRestarts  # ğŸ”¥ å…³é”®
  scheduler_params:
    T_0: 25
    T_mult: 2
    eta_min: 1e-6

  early_stopping_patience: 100
  epochs: 200
```

**é¢„æœŸæ•ˆæœ**:
1. âœ… Warm Restartsé¿å…é•¿æ—¶é—´ä½LR
2. âœ… å¼ºæ­£åˆ™åŒ–å‡å°‘è¿‡æ‹Ÿåˆå€¾å‘
3. âœ… Label Smoothingæé«˜æ³›åŒ–
4. âœ… Val F-Score: 0.695-0.705
5. âœ… Train-Val Gap < 5%

---

## ğŸ“Š å®éªŒéªŒè¯è®¡åˆ’

### å®éªŒ1: éªŒè¯Warm Restartsæ•ˆæœ

**å¯¹æ¯”**:
```bash
# Baseline (åŸé…ç½®)
Best: 0.6889 @ Epoch 92
Gap@92: 0.0098 (1.4%)
Gap@112: 0.0604 (9.3%)

# New (Warm Restarts)
é¢„æœŸBest: 0.695-0.700
é¢„æœŸGap: < 0.05 (5%)
```

**è¿è¡Œ**:
```bash
python main.py --config configs/lmsa_dice1.5_warm_restart.yaml
```

---

### å®éªŒ2: éªŒè¯æ›´é«˜eta_min

```yaml
# Test 1: eta_min = 1e-6 (åŸé…ç½®)
# Test 2: eta_min = 2e-4 (ä¸è®©LRé™å¤ªä½)

scheduler_params:
  eta_min: 2e-4  # æœ€ä½LR = 0.0002
```

**å‡è®¾**: eta_min=2e-4ä¼šå‡å°‘åæœŸè¿‡æ‹Ÿåˆ

---

## ğŸ“ ç†è®ºæ€»ç»“

### æ ¸å¿ƒç»“è®º

**å­¦ä¹ ç‡ä¸è¿‡æ‹Ÿåˆçš„å€’Uå‹å…³ç³»**:

```
Generalization
     â†‘
Good â”‚     â•±â•²
     â”‚    â•±  â•²       â† æœ€ä¼˜åŒºåŸŸ: LR=0.0006-0.0008
     â”‚   â•±    â•²
Poor â”‚  â•±      â•²â•²â•²â•²  â† LR<0.0006: å¼€å§‹è¿‡æ‹Ÿåˆ
     â”‚ â•±            â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Learning Rate
       Too High  Optimal  Too Low

å¤ªé«˜: è®­ç»ƒä¸ç¨³å®šï¼Œæ— æ³•æ”¶æ•›
æœ€ä¼˜: æ³›åŒ–èƒ½åŠ›æœ€å¼º
å¤ªä½: è®°å¿†è®­ç»ƒé›†ï¼Œè¿‡æ‹Ÿåˆ
```

### å®è·µå»ºè®®

1. **ä¸è¦è®©LRé™åˆ°åˆå§‹å€¼çš„50%ä»¥ä¸‹**
   - åŸé…ç½®: é™åˆ°71% â†’ è¿‡æ‹Ÿåˆ
   - å»ºè®®: ä½¿ç”¨Warm Restartsæˆ–æ›´é«˜eta_min

2. **ç›‘æ§Train-Val Gapï¼Œä¸åªæ˜¯Val F-Score**
   - Gap > 5%: è­¦å‘Š
   - Gap > 10%: ä¸¥é‡è¿‡æ‹Ÿåˆ

3. **é…åˆå¼ºæ­£åˆ™åŒ–**
   - ä½LRé˜¶æ®µéœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
   - Dropout 0.2, Weight Decay 2e-4

4. **Warm Restartsæ˜¯æœ€ä¼˜è§£**
   - é¿å…é•¿æ—¶é—´ä½LR
   - å¤šæ¬¡æ¢ç´¢æœºä¼š
   - è‡ªç„¶é˜²æ­¢è¿‡æ‹Ÿåˆ

---

## ğŸ”— ç›¸å…³æ–‡çŒ®

1. **"Cyclical Learning Rates for Training Neural Networks"**
   - Leslie Smith, 2017
   - è¯æ˜å‘¨æœŸæ€§LRä¼˜äºå•è°ƒé€’å‡

2. **"SGDR: Stochastic Gradient Descent with Warm Restarts"**
   - Loshchilov & Hutter, 2017
   - Warm Restartsç†è®ºåŸºç¡€

3. **"Sharp Minima Can Generalize For Deep Nets"**
   - äº‰è®ºsharp vs flat minima

---

**æœ€åæ›´æ–°**: 2025-10-09
**ç»“è®º**: å­¦ä¹ ç‡é™ä½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Warm Restartsæ˜¯æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚
