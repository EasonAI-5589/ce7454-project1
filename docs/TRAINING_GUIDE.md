# MicroSegFormer Training Guide

å®Œæ•´çš„MicroSegFormerè®­ç»ƒæµç¨‹æŒ‡å—ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æ£€æŸ¥æ•°æ®é›†
ls data/train/images/  # åº”è¯¥æœ‰1000å¼ è®­ç»ƒå›¾åƒ
ls data/train/masks/   # åº”è¯¥æœ‰å¯¹åº”çš„maskæ–‡ä»¶
```

### 2. å¼€å§‹è®­ç»ƒ

```bash
# æœ€ç®€å•çš„æ–¹å¼
./quick_start.sh train

# æˆ–ä½¿ç”¨Pythonç›´æ¥è¿è¡Œ
python main.py --config configs/main.yaml
```

## ğŸ“‹ è®­ç»ƒé…ç½®

### é»˜è®¤é…ç½® (configs/main.yaml)

```yaml
model:
  name: microsegformer
  num_classes: 19

data:
  root: data
  batch_size: 8
  num_workers: 4
  val_split: 0.1

training:
  epochs: 150
  optimizer: AdamW
  learning_rate: 1e-3
  weight_decay: 1e-4
  scheduler: CosineAnnealingLR
  warmup_epochs: 5
  early_stopping_patience: 20

loss:
  ce_weight: 1.0
  dice_weight: 0.5

augmentation:
  horizontal_flip: 0.5
  rotation: 15
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.1
  scale_range: [0.9, 1.1]
```

### è°ƒæ•´é…ç½®

ç›´æ¥ç¼–è¾‘ `configs/main.yaml` æ–‡ä»¶ï¼Œç„¶åé‡æ–°è®­ç»ƒã€‚

**å¸¸ç”¨è°ƒæ•´**:
```yaml
# å¢åŠ batch size (å¦‚æœGPUå†…å­˜å……è¶³)
batch_size: 16

# è°ƒæ•´å­¦ä¹ ç‡
learning_rate: 5e-4

# å»¶é•¿è®­ç»ƒ
epochs: 200

# è°ƒæ•´early stopping
early_stopping_patience: 30
```

## ğŸ¯ è®­ç»ƒå‘½ä»¤

### ä»å¤´è®­ç»ƒ

```bash
./quick_start.sh train
```

æˆ–ä½¿ç”¨æŒ‡å®šè®¾å¤‡ï¼š
```bash
python main.py --config configs/main.yaml --device cuda:0
```

### æ¢å¤è®­ç»ƒ

```bash
# ä»checkpointæ¢å¤
./quick_start.sh resume checkpoints/best_model.pth

# æˆ–æŒ‡å®šå®Œæ•´è·¯å¾„
python main.py \
  --config configs/main.yaml \
  --resume checkpoints/microsegformer_20241004_120000/best_model.pth
```

### è®­ç»ƒè¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
checkpoints/microsegformer_YYYYMMDD_HHMMSS/
â”œâ”€â”€ best_model.pth          # æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯é›†F-Scoreæœ€é«˜ï¼‰
â”œâ”€â”€ last_model.pth          # æœ€åä¸€ä¸ªepochçš„æ¨¡å‹
â”œâ”€â”€ config.yaml             # è®­ç»ƒé…ç½®å¤‡ä»½
â””â”€â”€ training_log.txt        # è®­ç»ƒæ—¥å¿—
```

checkpointä¸­åŒ…å«ï¼š
- `model_state_dict`: æ¨¡å‹æƒé‡
- `optimizer_state_dict`: ä¼˜åŒ–å™¨çŠ¶æ€
- `scheduler_state_dict`: å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
- `epoch`: å½“å‰epoch
- `best_f_score`: æœ€ä½³F-Score
- `config`: é…ç½®ä¿¡æ¯

## ğŸ” ç›‘æ§è®­ç»ƒ

### å®æ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

```bash
tail -f checkpoints/microsegformer_*/training_log.txt
```

### å…³é”®æŒ‡æ ‡

è®­ç»ƒæ—¶ä¼šè¾“å‡ºï¼š
- **Train Loss**: è®­ç»ƒæŸå¤±ï¼ˆCE Loss + Dice Lossï¼‰
- **Val F-Score**: éªŒè¯é›†F-Scoreï¼ˆä¸»è¦è¯„ä¼°æŒ‡æ ‡ï¼‰
- **Learning Rate**: å½“å‰å­¦ä¹ ç‡
- **Epoch Time**: æ¯ä¸ªepochè€—æ—¶

ç¤ºä¾‹è¾“å‡ºï¼š
```
Epoch [10/150] - Train Loss: 0.342, Val F-Score: 0.7856, LR: 9.5e-4, Time: 45.2s
Epoch [20/150] - Train Loss: 0.298, Val F-Score: 0.8123, LR: 8.8e-4, Time: 44.8s
```

## ğŸ“Š æµ‹è¯•æ¨¡å‹

### è¯„ä¼°éªŒè¯é›†

```bash
./quick_start.sh test checkpoints/best_model.pth

# æˆ–
python test.py --checkpoint checkpoints/best_model.pth
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
Loaded model from: checkpoints/best_model.pth
Epoch: 85
Best F-Score: 0.8234

Validation set size: 100
Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15<00:00]

Validation F-Score: 0.8234
```

## ğŸ’¡ è®­ç»ƒæŠ€å·§

### 1. æå‡æ€§èƒ½çš„ç­–ç•¥

**æ•°æ®å¢å¼º**:
```yaml
# å¢å¼ºæ›´æ¿€è¿›çš„æ•°æ®å¢å¼º
augmentation:
  horizontal_flip: 0.5
  rotation: 20              # å¢åŠ æ—‹è½¬è§’åº¦
  color_jitter:
    brightness: 0.3         # å¢åŠ äº®åº¦å˜åŒ–
    contrast: 0.3
    saturation: 0.2
  scale_range: [0.8, 1.2]   # å¢å¤§ç¼©æ”¾èŒƒå›´
```

**å­¦ä¹ ç‡è°ƒä¼˜**:
```yaml
# é™ä½åˆå§‹å­¦ä¹ ç‡
learning_rate: 5e-4

# æˆ–ä½¿ç”¨warmup
warmup_epochs: 10
```

**Lossæƒé‡è°ƒæ•´**:
```yaml
# å¢åŠ Dice Lossæƒé‡
loss:
  ce_weight: 1.0
  dice_weight: 1.0         # ä»0.5å¢åŠ åˆ°1.0
```

### 2. åŠ é€Ÿè®­ç»ƒ

**å¢åŠ batch size**:
```yaml
# å¦‚æœGPUå†…å­˜å…è®¸
batch_size: 16  # æˆ–32
```

**å‡å°‘éªŒè¯é¢‘ç‡**:
- ä¿®æ”¹ `src/trainer.py` ä¸­çš„éªŒè¯é¢‘ç‡
- æ¯2-3ä¸ªepochéªŒè¯ä¸€æ¬¡

**ä½¿ç”¨æ··åˆç²¾åº¦** (å¦‚æœæ”¯æŒ):
```yaml
training:
  use_amp: true
```

### 3. å¤„ç†è¿‡æ‹Ÿåˆ

å¦‚æœéªŒè¯F-Scoreä¸å†æå‡ï¼š

- å¢åŠ æ•°æ®å¢å¼ºå¼ºåº¦
- å¢åŠ weight decay: `1e-3`
- å‡å°‘æ¨¡å‹å®¹é‡ï¼ˆä½†è¦æ³¨æ„å‚æ•°é™åˆ¶ï¼‰
- ä½¿ç”¨early stoppingï¼ˆå·²å¯ç”¨ï¼‰

### 4. å¤„ç†æ¬ æ‹Ÿåˆ

å¦‚æœè®­ç»ƒå’ŒéªŒè¯F-Scoreéƒ½å¾ˆä½ï¼š

- å¢åŠ è®­ç»ƒepochs
- æé«˜å­¦ä¹ ç‡: `2e-3`
- æ£€æŸ¥æ•°æ®è´¨é‡
- å‡å°‘weight decay: `1e-5`

## ğŸ“ è®­ç»ƒæµç¨‹è¯´æ˜

### å®Œæ•´è®­ç»ƒæµç¨‹

1. **åˆå§‹åŒ–**
   - åŠ è½½é…ç½®
   - åˆ›å»ºæ¨¡å‹ï¼ˆéªŒè¯å‚æ•°é‡ï¼‰
   - å‡†å¤‡æ•°æ®é›†ï¼ˆ90% train / 10% valï¼‰
   - åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨

2. **è®­ç»ƒå¾ªç¯**
   ```
   for epoch in 1..150:
       - è®­ç»ƒä¸€ä¸ªepoch
       - è®¡ç®—è®­ç»ƒæŸå¤±
       - åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°F-Score
       - æ›´æ–°å­¦ä¹ ç‡ï¼ˆCosineAnnealingï¼‰
       - ä¿å­˜best modelï¼ˆå¦‚æœF-Scoreæå‡ï¼‰
       - æ£€æŸ¥early stopping
   ```

3. **Early Stopping**
   - å¦‚æœéªŒè¯F-Scoreè¿ç»­20ä¸ªepochæ²¡æœ‰æå‡
   - è‡ªåŠ¨åœæ­¢è®­ç»ƒ
   - é¿å…è¿‡æ‹Ÿåˆå’Œæµªè´¹æ—¶é—´

### å­¦ä¹ ç‡è°ƒåº¦

ä½¿ç”¨CosineAnnealingLRï¼š
- å¼€å§‹: 1e-3
- é€æ¸é™ä½åˆ°æ¥è¿‘0
- å‰5ä¸ªepochæœ‰warmup

### æŸå¤±å‡½æ•°

ç»„åˆæŸå¤±:
```python
total_loss = ce_loss * 1.0 + dice_loss * 0.5
```

- **CrossEntropy**: é€åƒç´ åˆ†ç±»
- **Dice Loss**: å…³æ³¨åŒºåŸŸé‡å 

## ğŸ› å¸¸è§é—®é¢˜

### CUDA Out of Memory
```bash
# å‡å°batch size
batch_size: 4
```

### è®­ç»ƒå¤ªæ…¢
```bash
# å‡å°‘workersæˆ–æ£€æŸ¥æ•°æ®åŠ è½½
num_workers: 2
```

### éªŒè¯F-Scoreä¸ç¨³å®š
- å¢åŠ éªŒè¯é›†å¤§å°: `val_split: 0.15`
- æˆ–ä½¿ç”¨å›ºå®šçš„éªŒè¯é›†

### æ¨¡å‹ä¸æ”¶æ•›
- æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–
- é™ä½å­¦ä¹ ç‡
- æ£€æŸ¥labelæ˜¯å¦æ­£ç¡®

## ğŸ“š ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆåï¼š
1. ä½¿ç”¨best_model.pthåœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹
2. æäº¤åˆ°Codabench
3. æ ¹æ®ç»“æœè°ƒæ•´é…ç½®
4. é‡æ–°è®­ç»ƒ
