# è¶…å‚æ•°ä¼˜åŒ–ç­–ç•¥

**å½“å‰æœ€ä½³**: Val F-Score 0.6889
**ä¼˜åŒ–ç›®æ ‡**: Val F-Score 0.70+

---

## ğŸ“Š å½“å‰æœ€ä½³é…ç½®åˆ†æ

### å·²éªŒè¯æœ€ä¼˜å‚æ•° âœ…

```yaml
loss:
  ce_weight: 1.0
  dice_weight: 1.5  # â­ å…³é”®å‚æ•°ï¼Œ1.0â†’1.5å¸¦æ¥+1.02%æå‡

training:
  learning_rate: 8e-4
  weight_decay: 1e-4
  batch_size: 32
  optimizer: AdamW
  scheduler: CosineAnnealingLR
  warmup_epochs: 5
  max_grad_norm: 1.0

model:
  dropout: 0.15

augmentation:
  horizontal_flip: 0.5
  rotation: 15  # ä¸è¦è¶…è¿‡15åº¦
  color_jitter: {brightness: 0.2, contrast: 0.2, saturation: 0.1}
  scale_range: [0.9, 1.1]
```

---

## ğŸ”¬ å¯ä¼˜åŒ–çš„è¶…å‚æ•°ç©ºé—´

### 1. æŸå¤±å‡½æ•°æƒé‡ â­â­â­â­â­ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

| å‚æ•° | å½“å‰å€¼ | å»ºè®®æµ‹è¯•èŒƒå›´ | é¢„æœŸå½±å“ | ä¼˜å…ˆçº§ |
|------|--------|-------------|----------|--------|
| **dice_weight** | 1.5 | **2.0, 2.5, 3.0** | +0.5-1.5% | â­â­â­â­â­ |
| ce_weight | 1.0 | ä¿æŒ1.0 | - | - |
| use_focal | False | âŒ å·²æµ‹è¯•ï¼Œ-1.7% | è´Ÿé¢ | âŒ |

**å®éªŒè®¾è®¡**:
```yaml
# Experiment 1: Dice 2.0
dice_weight: 2.0  # +0.3-0.6%é¢„æœŸ

# Experiment 2: Dice 2.5
dice_weight: 2.5  # +0.5-1.0%é¢„æœŸ

# Experiment 3: Dice 3.0 (æ¿€è¿›)
dice_weight: 3.0  # +0.2-0.8%é¢„æœŸï¼Œä½†å¯èƒ½è¿‡åº¦

# Experiment 4: åŠ¨æ€æƒé‡
dice_weight: 1.5 â†’ 2.5  # è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ 
```

**ç†ç”±**:
- Dice lossç›´æ¥ä¼˜åŒ–F-Scoreï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼‰
- å°ç›®æ ‡ï¼ˆçœ¼ç›ã€å˜´å·´ï¼‰å—ç›Šæœ€å¤§
- å†å²æ•°æ®è¯æ˜å•å‚æ•°æœ€å¤§æ”¶ç›Š

---

### 2. å­¦ä¹ ç‡ç­–ç•¥ â­â­â­â­

| å‚æ•° | å½“å‰å€¼ | å»ºè®®æµ‹è¯• | é¢„æœŸå½±å“ | ä¼˜å…ˆçº§ |
|------|--------|---------|----------|--------|
| **learning_rate** | 8e-4 | **6e-4, 1e-3** | +0.2-0.5% | â­â­â­â­ |
| warmup_epochs | 5 | **8, 10** | +0.1-0.3% | â­â­â­ |
| scheduler | CosineAnnealing | **CosineWarmRestarts** | +0.1-0.4% | â­â­â­ |

#### å®éªŒA: å­¦ä¹ ç‡æ‰«æ

```yaml
# Experiment 1: ç•¥ä½LRï¼ˆæ›´ç¨³å®šï¼‰
learning_rate: 6e-4
warmup_epochs: 8

# Experiment 2: ç•¥é«˜LRï¼ˆæ›´å¿«æ”¶æ•›ï¼‰
learning_rate: 1e-3
warmup_epochs: 10

# Experiment 3: æ›´é•¿warmup
learning_rate: 8e-4
warmup_epochs: 10
```

**ç†ç”±**:
- å½“å‰8e-4å¯èƒ½ä¸æ˜¯æœ€ä¼˜
- æ›´é•¿warmupå¯èƒ½å¸®åŠ©ç¨³å®šè®­ç»ƒ
- Face parsingæ˜¯ç²¾ç»†ä»»åŠ¡ï¼Œå¯èƒ½å—ç›Šäºæ›´ä½LR

#### å®éªŒB: Cosine Warm Restarts

```yaml
scheduler: CosineAnnealingWarmRestarts
scheduler_params:
  T_0: 30      # æ¯30è½®é‡å¯
  T_mult: 2    # é‡å¯å‘¨æœŸç¿»å€
  eta_min: 1e-6
```

**ä¼˜åŠ¿**:
- å‘¨æœŸæ€§é‡å¯å¯èƒ½è·³å‡ºå±€éƒ¨æœ€ä¼˜
- å¤šæ¬¡æ”¶æ•›æœºä¼š
- å¯¹é•¿è®­ç»ƒ(200 epochs)æœ‰å¸®åŠ©

**é¢„æœŸ**: +0.1-0.4%

---

### 3. æ­£åˆ™åŒ–å‚æ•° â­â­â­

| å‚æ•° | å½“å‰å€¼ | å»ºè®®æµ‹è¯• | é¢„æœŸå½±å“ | ä¼˜å…ˆçº§ |
|------|--------|---------|----------|--------|
| **dropout** | 0.15 | **0.1, 0.2, 0.25** | +0.1-0.3% | â­â­â­ |
| **weight_decay** | 1e-4 | **5e-5, 2e-4, 3e-4** | +0.1-0.3% | â­â­â­ |
| max_grad_norm | 1.0 | 0.5, 2.0 | +0.0-0.2% | â­â­ |

#### å®éªŒè®¾è®¡

**ç»„åˆ1: è½»æ­£åˆ™åŒ–**ï¼ˆæ›´é«˜å®¹é‡ï¼‰
```yaml
dropout: 0.1
weight_decay: 5e-5
```
- é€‚åˆï¼šæ•°æ®é‡å°(1000å¼ )ï¼Œæ¨¡å‹éœ€è¦è®°å¿†æ›´å¤šç»†èŠ‚
- é£é™©ï¼šè½»å¾®è¿‡æ‹Ÿåˆ

**ç»„åˆ2: å¼ºæ­£åˆ™åŒ–**ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
```yaml
dropout: 0.25
weight_decay: 3e-4
```
- é€‚åˆï¼šé˜²æ­¢åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ
- é£é™©ï¼šæ¬ æ‹Ÿåˆ

**ç»„åˆ3: å¹³è¡¡**ï¼ˆå½“å‰+è°ƒæ•´ï¼‰
```yaml
dropout: 0.2
weight_decay: 2e-4
```
- å¹³è¡¡æ–¹æ¡ˆï¼Œç•¥å¢å¼ºæ­£åˆ™åŒ–

**é¢„æœŸ**: +0.1-0.3%

---

### 4. Batch Sizeä¸ä¼˜åŒ–å™¨å‚æ•° â­â­â­

| å‚æ•° | å½“å‰å€¼ | å»ºè®®æµ‹è¯• | é¢„æœŸå½±å“ | ä¼˜å…ˆçº§ |
|------|--------|---------|----------|--------|
| **batch_size** | 32 | **24, 40, 48** | +0.1-0.4% | â­â­â­ |
| optimizer | AdamW | **AdamW + lookahead** | +0.1-0.3% | â­â­ |

#### Batch Sizeå®éªŒ

**å°Batch (24)**:
```yaml
batch_size: 24
learning_rate: 6e-4  # å¯¹åº”è°ƒä½
```
- ä¼˜ç‚¹ï¼šæ›´å¤šæ¢¯åº¦æ›´æ–°ï¼Œæ›´å¼ºæ­£åˆ™åŒ–æ•ˆæœ
- ç¼ºç‚¹ï¼šè®­ç»ƒæ—¶é—´+25%

**å¤§Batch (40-48)**:
```yaml
batch_size: 40
learning_rate: 1e-3  # å¯¹åº”è°ƒé«˜
```
- ä¼˜ç‚¹ï¼šè®­ç»ƒæ›´ç¨³å®šï¼Œé€Ÿåº¦æ›´å¿«
- ç¼ºç‚¹ï¼šæ³›åŒ–æ€§å¯èƒ½ç•¥å·®

**é¢„æœŸ**: +0.1-0.4%

#### AdamWå‚æ•°è°ƒæ•´

```yaml
optimizer: AdamW
optimizer_params:
  betas: [0.9, 0.999]  # å½“å‰
  # æµ‹è¯•æ›´é«˜momentum
  betas: [0.95, 0.999]  # æ›´å¹³æ»‘çš„ä¼˜åŒ–è·¯å¾„
```

**é¢„æœŸ**: +0.0-0.2%

---

### 5. æ•°æ®å¢å¼ºå¾®è°ƒ â­â­

| å‚æ•° | å½“å‰å€¼ | å»ºè®®æµ‹è¯• | é¢„æœŸå½±å“ | ä¼˜å…ˆçº§ |
|------|--------|---------|----------|--------|
| rotation | 15 | **10, 12** | +0.0-0.2% | â­â­ |
| scale_range | [0.9,1.1] | **[0.85,1.15], [0.95,1.05]** | +0.0-0.2% | â­â­ |
| horizontal_flip | 0.5 | **0.6, 0.7** | +0.0-0.1% | â­ |

**æ³¨æ„**:
- âš ï¸ å·²éªŒè¯è¿‡åº¦å¢å¼ºä¼šé™ä½æ€§èƒ½(-0.5%)
- ä»…å»ºè®®å¾®å°è°ƒæ•´
- **ä¸è¦è¶…è¿‡**: rotation 15Â°, color_jitter 0.2

#### ä¿å®ˆè°ƒæ•´æ–¹æ¡ˆ

```yaml
augmentation:
  horizontal_flip: 0.6  # ç¨å¾®å¢åŠ 
  rotation: 12          # ç¨å¾®å‡å°‘ï¼ˆæ›´ä¿å®ˆï¼‰
  scale_range: [0.92, 1.08]  # æ›´ä¿å®ˆçš„èŒƒå›´
  color_jitter:
    brightness: 0.15    # ç¨å¾®å‡å°‘
    contrast: 0.15      # ç¨å¾®å‡å°‘
    saturation: 0.1     # ä¿æŒ
```

**é¢„æœŸ**: +0.0-0.2%

---

### 6. é«˜çº§æŠ€å·§ â­â­â­

#### A. Label Smoothing

```yaml
loss:
  label_smoothing: 0.1  # å¹³æ»‘one-hotæ ‡ç­¾
```

**åŸç†**:
- å°†hard label [0,1] å˜ä¸º [0.05, 0.95]
- é˜²æ­¢è¿‡åº¦è‡ªä¿¡ï¼Œæé«˜æ³›åŒ–

**é¢„æœŸ**: +0.1-0.3%

#### B. Mixup / CutMix (è°¨æ…ä½¿ç”¨)

```yaml
augmentation:
  mixup_alpha: 0.2  # æ··åˆä¸¤å¼ å›¾åƒ
  # æˆ–
  cutmix_alpha: 1.0  # è£å‰ªç²˜è´´
```

**æ³¨æ„**: Face parsingå¯èƒ½ä¸é€‚åˆMixupï¼ˆç ´åé¢éƒ¨ç»“æ„ï¼‰

**é¢„æœŸ**: -0.2 to +0.2% (ä¸ç¡®å®š)

#### C. Stochastic Depth (é’ˆå¯¹Decoder)

```yaml
model:
  stochastic_depth: 0.1  # éšæœºä¸¢å¼ƒå±‚
```

**ä¼˜ç‚¹**: ç±»ä¼¼dropoutçš„æ­£åˆ™åŒ–
**é¢„æœŸ**: +0.1-0.2%

---

## ğŸ¯ æ¨èå®éªŒè®¡åˆ’

### Tier 1: é«˜ä¼˜å…ˆçº§ï¼ˆå¿…åšï¼‰â­â­â­â­â­

| å®éªŒID | é…ç½®å˜åŒ– | é¢„æœŸæå‡ | GPUæ—¶é—´ |
|--------|---------|---------|---------|
| **E1** | Dice=2.0 | +0.3-0.6% | 6h |
| **E2** | Dice=2.5 | +0.5-1.0% | 6h |
| **E3** | Dice=2.5 + dropout=0.2 + wd=2e-4 | +0.6-1.2% | 6h |
| **E4** | Dice=3.0 | +0.2-0.8% | 6h |

**æ€»æ—¶é—´**: 24å°æ—¶
**é¢„æœŸæœ€ä½³æ”¶ç›Š**: +0.6-1.2%

---

### Tier 2: ä¸­ä¼˜å…ˆçº§ï¼ˆå»ºè®®ï¼‰â­â­â­â­

| å®éªŒID | é…ç½®å˜åŒ– | é¢„æœŸæå‡ | GPUæ—¶é—´ |
|--------|---------|---------|---------|
| **E5** | LR=6e-4 + warmup=8 | +0.2-0.5% | 6h |
| **E6** | LR=1e-3 + warmup=10 | +0.2-0.5% | 6h |
| **E7** | CosineWarmRestarts | +0.1-0.4% | 6h |
| **E8** | Label Smoothing=0.1 | +0.1-0.3% | 6h |

**æ€»æ—¶é—´**: 24å°æ—¶
**é¢„æœŸæœ€ä½³æ”¶ç›Š**: +0.2-0.5%

---

### Tier 3: ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰â­â­

| å®éªŒID | é…ç½®å˜åŒ– | é¢„æœŸæå‡ | GPUæ—¶é—´ |
|--------|---------|---------|---------|
| E9 | Batch=24 + LR=6e-4 | +0.1-0.3% | 8h |
| E10 | Batch=40 + LR=1e-3 | +0.1-0.3% | 5h |
| E11 | Dropout sweep (0.1,0.2,0.25) | +0.1-0.3% | 18h |

**æ€»æ—¶é—´**: 31å°æ—¶
**é¢„æœŸæœ€ä½³æ”¶ç›Š**: +0.1-0.3%

---

## ğŸ“‹ æœ€ä¼˜æ‰§è¡Œç­–ç•¥

### ç­–ç•¥A: å¿«é€Ÿè¿­ä»£ï¼ˆæ¨èï¼‰

**æ—¶é—´**: 3å¤©
**ç›®æ ‡**: Val 0.70+

```bash
# Day 1: Dice weightæ‰«æ
python main.py --config configs/lmsa_dice2.0_fresh.yaml
python main.py --config configs/lmsa_dice2.5_aggressive.yaml

# Day 2: æœ€ä½³Dice + LRä¼˜åŒ–
# å‡è®¾Dice 2.5æœ€å¥½ï¼Œåˆ›å»ºæ–°config
python main.py --config configs/lmsa_dice2.5_lr6e-4.yaml

# Day 3: æœ€ä½³ç»„åˆ + Label Smoothing
python main.py --config configs/lmsa_dice2.5_final.yaml
```

**é¢„æœŸç»“æœ**: Val 0.695-0.705

---

### ç­–ç•¥B: ç½‘æ ¼æœç´¢ï¼ˆå…¨é¢ï¼‰

**æ—¶é—´**: 5-7å¤©
**ç›®æ ‡**: æ‰¾åˆ°å…¨å±€æœ€ä¼˜

```python
# ç½‘æ ¼æœç´¢é…ç½®
grid = {
    'dice_weight': [2.0, 2.5, 3.0],
    'learning_rate': [6e-4, 8e-4, 1e-3],
    'dropout': [0.1, 0.15, 0.2],
    'weight_decay': [1e-4, 2e-4]
}

# æ€»ç»„åˆ: 3 Ã— 3 Ã— 3 Ã— 2 = 54ä¸ª
# æ™ºèƒ½å‰ªæå: ~15-20ä¸ªå…³é”®ç»„åˆ
```

**ä¸æ¨è**: æ—¶é—´æˆæœ¬å¤ªé«˜

---

## ğŸ”§ å¿«é€Ÿé…ç½®ç”Ÿæˆå™¨

### é…ç½®1: Dice 2.5 + å¼ºæ­£åˆ™åŒ–ï¼ˆæ¨èï¼‰

```yaml
# configs/lmsa_dice2.5_strong_reg.yaml
loss:
  dice_weight: 2.5

model:
  dropout: 0.2

training:
  learning_rate: 6e-4
  weight_decay: 2e-4
  warmup_epochs: 8
```

**é¢„æœŸ**: Val 0.695-0.705

---

### é…ç½®2: Dice 2.0 + ä¼˜åŒ–LR

```yaml
# configs/lmsa_dice2.0_optim_lr.yaml
loss:
  dice_weight: 2.0

training:
  learning_rate: 1e-3
  warmup_epochs: 10
  scheduler: CosineAnnealingWarmRestarts
```

**é¢„æœŸ**: Val 0.690-0.700

---

### é…ç½®3: Dice 2.5 + Label Smoothing

```yaml
# configs/lmsa_dice2.5_label_smooth.yaml
loss:
  dice_weight: 2.5
  label_smoothing: 0.1

model:
  dropout: 0.2

training:
  learning_rate: 6e-4
  weight_decay: 2e-4
```

**é¢„æœŸ**: Val 0.697-0.707

---

## ğŸ“Š é¢„æœŸæå‡è·¯å¾„

```
å½“å‰æœ€ä½³: 0.6889
  â†“ +Dice 2.5
0.694-0.699
  â†“ +LRä¼˜åŒ–
0.696-0.701
  â†“ +Label Smoothing
0.698-0.703
  â†“ +æ­£åˆ™åŒ–è°ƒæ•´
0.700-0.705 â† ç›®æ ‡

Teståˆ†æ•°é¢„æœŸ: 0.74-0.75
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### ä¸è¦åšçš„äº‹æƒ…

1. âŒ **è¿‡åº¦å¢å¼ºæ•°æ®**: rotation > 15Â°, color_jitter > 0.2
2. âŒ **ä½¿ç”¨Focal Loss**: å·²éªŒè¯-1.7%
3. âŒ **æ”¹å˜Batch Sizeåˆ°æç«¯å€¼**: <16 æˆ– >64
4. âŒ **åŒæ—¶æ”¹å˜å¤ªå¤šå‚æ•°**: éš¾ä»¥å½’å› 

### å®éªŒæœ€ä½³å®è·µ

1. âœ… **å•å˜é‡å¯¹ç…§**: æ¯æ¬¡åªæ”¹1-2ä¸ªå‚æ•°
2. âœ… **è®°å½•æ‰€æœ‰ç»“æœ**: åŒ…æ‹¬å¤±è´¥çš„å®éªŒ
3. âœ… **å¤šæ¬¡éªŒè¯**: é‡è¦é…ç½®è·‘2-3æ¬¡ç¡®è®¤
4. âœ… **Early stopping patience=100**: å……åˆ†è®­ç»ƒ

---

## ğŸ’¡ æœ€ç»ˆå»ºè®®

### ç«‹å³æ‰§è¡Œï¼ˆå·²å‡†å¤‡ï¼‰

```bash
# Tier 1å®éªŒå·²é…ç½®å®Œæˆ
python main.py --config configs/lmsa_dice2.5_aggressive.yaml  # E3
python main.py --config configs/lmsa_dice2.0_fresh.yaml       # E1
```

### ä¸‹ä¸€æ­¥å‡†å¤‡

å¦‚æœDice 2.5è¾¾åˆ°0.695-0.700ï¼Œåˆ›å»ºé…ç½®ï¼š
```yaml
# configs/lmsa_dice2.5_final.yaml
loss:
  dice_weight: 2.5
  label_smoothing: 0.1  # æ–°å¢

model:
  dropout: 0.2

training:
  learning_rate: 6e-4
  weight_decay: 2e-4
  warmup_epochs: 8
  scheduler: CosineAnnealingWarmRestarts
  scheduler_params:
    T_0: 30
```

**é¢„æœŸ**: Val 0.70-0.71 â†’ Test 0.74-0.75

---

**æ€»ç»“**: è¶…å‚æ•°ä¼˜åŒ–ç©ºé—´å·¨å¤§ï¼Œä¼˜å…ˆå®ŒæˆDice weightå®éªŒåï¼Œå†é€æ­¥ä¼˜åŒ–LRã€æ­£åˆ™åŒ–ã€Label Smoothingç­‰ã€‚

**æœ€åæ›´æ–°**: 2025-10-09
