# å­¦ä¹ ç‡ä¼˜åŒ–é…ç½®æ–¹æ¡ˆ

**é—®é¢˜**: å½“å‰æœ€ä½³æ¨¡å‹(0.6889)åœ¨Epoch 92è¾¾åˆ°æœ€ä¼˜åï¼ŒLRå·²é™è‡³åˆå§‹å€¼çš„80%ï¼Œåç»­20è½®æ— æå‡

**æ ¹æœ¬åŸå› **:
- CosineAnnealing LRè¡°å‡è¿‡å¿«
- Epoch 92æ—¶LR=0.00064ï¼Œå·²ç»è¿‡ä½
- éš¾ä»¥ç»§ç»­ä¼˜åŒ–ï¼Œé™·å…¥å±€éƒ¨æœ€ä¼˜

---

## ğŸ¯ 4ä¸ªå­¦ä¹ ç‡ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: Dice 1.5 + Warm Restarts â­â­â­â­â­ï¼ˆæ¨èï¼‰

**æ–‡ä»¶**: `configs/lmsa_dice1.5_warm_restart.yaml`

**æ ¸å¿ƒæ”¹åŠ¨**:
```yaml
scheduler: CosineAnnealingWarmRestarts
scheduler_params:
  T_0: 25       # ç¬¬ä¸€æ¬¡é‡å¯åœ¨25è½®
  T_mult: 2     # ä¹‹åå‘¨æœŸç¿»å€ï¼š25, 50, 100
  eta_min: 1e-6
```

**åŸç†**:
- å‘¨æœŸæ€§é‡å¯å­¦ä¹ ç‡ï¼Œä»é«˜åˆ°ä½å†é‡å¯
- æ¯æ¬¡é‡å¯æ˜¯ä¸€æ¬¡æ–°çš„æ¢ç´¢æœºä¼š
- T_0=25å¯¹åº”åŸæ¨¡å‹Epoch 92é™„è¿‘ï¼ˆç¬¬ä¸€æ¬¡æ”¶æ•›ç‚¹ï¼‰
- T_mult=2è®©åç»­å‘¨æœŸæ›´é•¿ï¼Œç¨³å®šæ”¶æ•›

**LRæ›²çº¿ç¤ºæ„**:
```
Epoch:  0----25----50-----100-----200
LR:     8e-4 â†“1e-6â†‘8e-4 â†“1e-6â†‘8e-4 â†“1e-6
        â””â”€warmâ”€â”˜  â””â”€â”€restart 1â”€â”€â”˜  â””â”€restart 2â”€â”˜
```

**é¢„æœŸ**:
- Val F-Score: **0.690-0.695** (+0.1-0.6%)
- å¤šæ¬¡æ”¶æ•›æœºä¼šï¼Œå¯èƒ½æ‰¾åˆ°æ›´å¥½çš„è§£
- é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜

---

### æ–¹æ¡ˆ2: Dice 1.5 + Slower Decay â­â­â­â­

**æ–‡ä»¶**: `configs/lmsa_dice1.5_slow_decay.yaml`

**æ ¸å¿ƒæ”¹åŠ¨**:
```yaml
scheduler: CosineAnnealingLR
scheduler_params:
  T_max: 150    # ğŸ”¥ è®¾ä¸º150è€Œä¸æ˜¯200ï¼Œè®©LRè¡°å‡æ›´æ…¢
  eta_min: 1e-6
```

**åŸç†**:
- åŸé…ç½®T_max=200ï¼ŒEpoch 92æ—¶LRå·²é™è‡³80%
- æ–°é…ç½®T_max=150ï¼ŒEpoch 92æ—¶LRé™è‡³~90%
- **ç»™åæœŸè®­ç»ƒç•™æ›´å¤šä¼˜åŒ–ç©ºé—´**

**LRå¯¹æ¯”**:
```
Epoch:      60    92    120   150   200
åŸé…ç½®LR:   92%   80%   70%   62%   57%
æ–°é…ç½®LR:   96%   90%   84%   76%   68%
å¢å¹…:       +4%   +10%  +14%  +14%  +11%
```

**é¢„æœŸ**:
- Val F-Score: **0.689-0.693** (+0.0-0.4%)
- æ›´å¹³ç¼“çš„è¡°å‡ï¼ŒåæœŸä»å¯ä¼˜åŒ–
- ä¿å®ˆä½†ç¨³å¦¥

---

### æ–¹æ¡ˆ3: Dice 1.5 + Higher LR â­â­â­

**æ–‡ä»¶**: `configs/lmsa_dice1.5_higher_lr.yaml`

**æ ¸å¿ƒæ”¹åŠ¨**:
```yaml
learning_rate: 1e-3  # ğŸ”¥ ä»8e-4æå‡åˆ°1e-3 (+25%)
warmup_epochs: 10    # æ›´é•¿warmupç¨³å®šè®­ç»ƒ

scheduler: CosineAnnealingLR
scheduler_params:
  T_max: 150
```

**åŸç†**:
- æ›´é«˜åˆå§‹LRå¯èƒ½æ‰¾åˆ°æ›´å¥½çš„ä¼˜åŒ–è·¯å¾„
- æ›´é•¿warmup(10è½®)é˜²æ­¢è®­ç»ƒä¸ç¨³å®š
- é…åˆæ…¢è¡°å‡ç­–ç•¥

**é£é™©**:
- âš ï¸ æ›´é«˜LRå¯èƒ½å¯¼è‡´è®­ç»ƒæ—©æœŸéœ‡è¡
- âš ï¸ éœ€è¦æ›´å¼ºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ

**é¢„æœŸ**:
- Val F-Score: **0.688-0.696** (+0.0-0.7%)
- æ›´å¿«æ”¶æ•›ï¼Œå¯èƒ½æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½
- æˆ–è€…æ›´æ—©è¿‡æ‹Ÿåˆ

---

### æ–¹æ¡ˆ4: Dice 2.5 + ç»ˆæLRä¼˜åŒ– â­â­â­â­â­ï¼ˆæœ€æ¿€è¿›ï¼‰

**æ–‡ä»¶**: `configs/lmsa_dice2.5_lr_optimized.yaml`

**æ ¸å¿ƒæ”¹åŠ¨**:
```yaml
loss:
  dice_weight: 2.5  # æ¿€è¿›ä¼˜åŒ–å°ç›®æ ‡

model:
  dropout: 0.2      # æ›´å¼ºæ­£åˆ™åŒ–

training:
  learning_rate: 8e-4
  weight_decay: 2e-4
  scheduler: CosineAnnealingWarmRestarts
  scheduler_params:
    T_0: 30
    T_mult: 2
  warmup_epochs: 8  # ç»™Dice 2.5æ›´å¤šçƒ­èº«æ—¶é—´
```

**ç»„åˆç­–ç•¥**:
1. **Dice 2.5**: æœ€æ¿€è¿›çš„å°ç›®æ ‡ä¼˜åŒ–
2. **Warm Restarts**: è§£å†³LRè¡°å‡è¿‡å¿«
3. **å¼ºæ­£åˆ™åŒ–**: dropout 0.2 + wd 2e-4é˜²æ­¢è¿‡æ‹Ÿåˆ
4. **é•¿warmup**: ç»™å¤æ‚æŸå¤±å‡½æ•°æ›´å¤šé€‚åº”æ—¶é—´

**é¢„æœŸ**:
- Val F-Score: **0.700-0.710** (+1.1-2.1%)
- Test F-Score: **0.74-0.75**
- **å†²å‡»Topæ’å**

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”è¡¨

| æ–¹æ¡ˆ | Dice | Scheduler | åˆå§‹LR | å…³é”®æ”¹åŠ¨ | é¢„æœŸVal | ä¼˜å…ˆçº§ |
|------|------|-----------|--------|---------|---------|--------|
| 1. Warm Restart | 1.5 | WarmRestarts | 8e-4 | å‘¨æœŸæ€§é‡å¯ | 0.690-0.695 | â­â­â­â­â­ |
| 2. Slow Decay | 1.5 | CosineAnneal | 8e-4 | T_max=150 | 0.689-0.693 | â­â­â­â­ |
| 3. Higher LR | 1.5 | CosineAnneal | 1e-3 | LR +25% | 0.688-0.696 | â­â­â­ |
| 4. Ultimate | 2.5 | WarmRestarts | 8e-4 | å…¨æ–¹ä½ä¼˜åŒ– | 0.700-0.710 | â­â­â­â­â­ |

---

## ğŸ”§ æŠ€æœ¯å®ç°

### ä»£ç æ›´æ–°

**src/trainer.py**:
```python
# æ–°å¢æ”¯æŒCosineAnnealingWarmRestarts
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

def create_scheduler(optimizer, config, num_epochs):
    scheduler_type = config.get('scheduler', 'CosineAnnealingLR')
    scheduler_params = config.get('scheduler_params', {})

    if scheduler_type == 'CosineAnnealingWarmRestarts':
        T_0 = scheduler_params.get('T_0', 30)
        T_mult = scheduler_params.get('T_mult', 2)
        eta_min = scheduler_params.get('eta_min', 0)
        main_scheduler = CosineAnnealingWarmRestarts(
            optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min
        )
```

### é…ç½®ç¤ºä¾‹

**Warm Restartsé…ç½®**:
```yaml
training:
  scheduler: CosineAnnealingWarmRestarts
  scheduler_params:
    T_0: 25        # é¦–æ¬¡é‡å¯å‘¨æœŸ
    T_mult: 2      # å‘¨æœŸå€å¢å› å­
    eta_min: 1e-6  # æœ€å°å­¦ä¹ ç‡
```

**Slow Decayé…ç½®**:
```yaml
training:
  scheduler: CosineAnnealingLR
  scheduler_params:
    T_max: 150     # è‡ªå®šä¹‰æ€»å‘¨æœŸ
    eta_min: 1e-6
```

---

## ğŸ“ˆ é¢„æœŸå­¦ä¹ ç‡æ›²çº¿

### åŸé…ç½® (T_max=200)
```
LR
0.0008 â”¤
0.0007 â”¤â”€â”€â”€â•®
0.0006 â”¤     â•°â”€â”€â•®        â† Epoch 92 (best)
0.0005 â”¤         â•°â”€â”€â•®
0.0004 â”¤            â•°â”€â”€â•®
0.0003 â”¤               â•°â”€â”€â•®
0.0002 â”¤                  â•°â”€â”€â•®
0.0001 â”¤                     â•°â”€â”€â•®
0.0000 â”¤                        â•°â”€â”€â”€â”€â”€â”€â”€â”€
       0   50   100  150  200  Epoch

é—®é¢˜ï¼šEpoch 92åLRå·²é™è‡³0.00064ï¼Œéš¾ä»¥ç»§ç»­ä¼˜åŒ–
```

### æ–¹æ¡ˆ1: Warm Restarts (T_0=25, T_mult=2)
```
LR
0.0008 â”¤â”€â”€â•®   â•­â”€â”€â•®        â•­â”€â”€â•®
0.0007 â”¤   â•² â•±   â•²      â•±   â•²
0.0006 â”¤    â•³     â•²    â•±     â•²
0.0005 â”¤   â•± â•²     â•²  â•±       â•²
0.0004 â”¤  â•±   â•²     â•²â•±         â•²
0.0003 â”¤ â•±     â•²                â•²
0.0002 â”¤â•±       â•²                â•²
0.0001 â”¤         â•²                â•²
0.0000 â”¤          â•²                â•²
       0   25   50    100     200  Epoch
           â†‘restart  â†‘restart

ä¼˜åŠ¿ï¼šå¤šæ¬¡æ¢ç´¢æœºä¼šï¼Œé¿å…å±€éƒ¨æœ€ä¼˜
```

### æ–¹æ¡ˆ2: Slow Decay (T_max=150)
```
LR
0.0008 â”¤
0.0007 â”¤â”€â”€â”€â”€â•®
0.0006 â”¤     â•°â”€â”€â”€â•®     â† Epoch 92 (more LR left!)
0.0005 â”¤         â•°â”€â”€â”€â•®
0.0004 â”¤             â•°â”€â”€â”€â•®
0.0003 â”¤                 â•°â”€â”€â”€â•®
0.0002 â”¤                     â•°â”€â”€â”€â•®
0.0001 â”¤                         â•°â”€â”€â”€â•®
0.0000 â”¤                             â•°â”€â”€â”€
       0   50   100  150  200  Epoch

ä¼˜åŠ¿ï¼šEpoch 92æ—¶LR=0.00072 (vs åŸæ¥0.00064)
      åæœŸä»æœ‰ä¼˜åŒ–ç©ºé—´
```

---

## ğŸ¯ æ¨èå®éªŒé¡ºåº

### Week 1 (é«˜ä¼˜å…ˆçº§)

**Day 1-3**: æ–¹æ¡ˆ1 (Warm Restarts, Dice 1.5)
```bash
python main.py --config configs/lmsa_dice1.5_warm_restart.yaml
```
- æœ€æœ‰æ½œåŠ›è§£å†³LRé—®é¢˜
- é£é™©ä½ï¼Œç¨³å®šæ€§å¥½
- é¢„æœŸ: Val 0.690-0.695

**Day 4-6**: æ–¹æ¡ˆ4 (Ultimate, Dice 2.5)
```bash
python main.py --config configs/lmsa_dice2.5_lr_optimized.yaml
```
- ç»„åˆæœ€ä¼˜ç­–ç•¥
- å†²å‡»0.70+
- é¢„æœŸ: Val 0.700-0.710

---

### Week 2 (å¦‚æœéœ€è¦)

**Day 7-9**: æ–¹æ¡ˆ2 (Slow Decay)
```bash
python main.py --config configs/lmsa_dice1.5_slow_decay.yaml
```
- ä¿å®ˆæ–¹æ¡ˆ
- éªŒè¯æ…¢è¡°å‡æ•ˆæœ

**Day 10-12**: æ–¹æ¡ˆ3 (Higher LR)
```bash
python main.py --config configs/lmsa_dice1.5_higher_lr.yaml
```
- æ¢ç´¢æ›´é«˜LRè·¯å¾„
- å¯èƒ½æ›´å¿«æ”¶æ•›

---

## ğŸ’¡ ä¸ºä»€ä¹ˆWarm Restartsæœ€æœ‰æ½œåŠ›ï¼Ÿ

### ç†è®ºä¼˜åŠ¿

1. **é€ƒå‡ºå±€éƒ¨æœ€ä¼˜**:
   - å‘¨æœŸæ€§é‡å¯æä¾›"é‡æ–°å¼€å§‹"çš„æœºä¼š
   - é«˜LRé˜¶æ®µå¯ä»¥è·³å‡ºä¹‹å‰çš„å±€éƒ¨æœ€ä¼˜

2. **å¤šæ¬¡æ”¶æ•›æœºä¼š**:
   - æ¯ä¸ªå‘¨æœŸéƒ½æ˜¯ä¸€æ¬¡å®Œæ•´çš„ä¼˜åŒ–è¿‡ç¨‹
   - 200è½®è®­ç»ƒç›¸å½“äº3-4æ¬¡ç‹¬ç«‹å°è¯•

3. **æ¢ç´¢-åˆ©ç”¨å¹³è¡¡**:
   - é«˜LRé˜¶æ®µï¼šæ¢ç´¢æ–°è§£
   - ä½LRé˜¶æ®µï¼šç²¾ç»†ä¼˜åŒ–
   - åŠ¨æ€å¹³è¡¡

### å®è¯æ”¯æŒ

**SGDRè®ºæ–‡ç»“æœ**:
- CIFAR-10: +0.5-1.0% improvement
- ImageNet: +0.3-0.5% improvement
- **Face Parsingç±»ä¼¼ä»»åŠ¡é¢„æœŸ**: +0.2-0.6%

**æˆ‘ä»¬çš„æƒ…å†µ**:
- å½“å‰å¡åœ¨0.6889
- Epoch 92åLRè¿‡ä½
- **Warm Restartså¯èƒ½çªç ´ç“¶é¢ˆ**

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### Warm Restartsä½¿ç”¨å»ºè®®

1. **Early Stoppingè¦å°å¿ƒ**:
   - Patienceè¦è¶³å¤Ÿå¤§(100+)
   - å¦åˆ™å¯èƒ½åœ¨é‡å¯å‰å°±åœæ­¢

2. **é‡å¯å‘¨æœŸé€‰æ‹©**:
   - T_0å¤ªå°ï¼šé¢‘ç¹é‡å¯ï¼Œä¸ç¨³å®š
   - T_0å¤ªå¤§ï¼šé€€åŒ–ä¸ºæ™®é€šCosineAnnealing
   - **å»ºè®®**: T_0 = åŸæ”¶æ•›è½®æ•°çš„70-80%

3. **Validationæ³¢åŠ¨**:
   - é‡å¯æ—¶æ€§èƒ½å¯èƒ½æš‚æ—¶ä¸‹é™
   - æ­£å¸¸ç°è±¡ï¼Œä¸è¦ææ…Œ

### Higher LRé£é™©

1. **è®­ç»ƒä¸ç¨³å®š**:
   - æ¢¯åº¦å¯èƒ½çˆ†ç‚¸
   - éœ€è¦æ›´å¼ºmax_grad_norm

2. **æ—©æœŸè¿‡æ‹Ÿåˆ**:
   - éœ€è¦æ›´å¼ºæ­£åˆ™åŒ–
   - Dropout 0.2, Weight Decay 2e-4

---

## ğŸ“ ç»éªŒæ•™è®­

### Lesson 1: LR Schedulerè¦åŒ¹é…è®­ç»ƒé•¿åº¦

- **é”™è¯¯**: è®¾ç½®epochs=200ä½†åœ¨100è½®å°±æ”¶æ•›
- **é—®é¢˜**: ScheduleræŒ‰200è½®è¡°å‡ï¼ŒLRé™å¤ªæ…¢
- **æ­£ç¡®**: T_maxè®¾ä¸ºé¢„æœŸæ”¶æ•›è½®æ•°çš„1.2-1.5å€

### Lesson 2: Early Stoppingè¦å®½æ¾

- **é”™è¯¯**: Patience=20
- **é—®é¢˜**: Warm Restartséœ€è¦æ›´å¤šæ¢ç´¢æ—¶é—´
- **æ­£ç¡®**: Patience=50-100

### Lesson 3: ç›‘æ§LRå¾ˆé‡è¦

- **å·¥å…·**: TensorBoard, Wandb
- **å…³é”®æŒ‡æ ‡**: å½“å‰LR, Val F-Score, Train-Val Gap
- **åˆ¤æ–­**: LR < 1e-4æ—¶åŸºæœ¬æ— æ³•ç»§ç»­ä¼˜åŒ–

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **SGDR: Stochastic Gradient Descent with Warm Restarts**
   - Loshchilov & Hutter, ICLR 2017
   - https://arxiv.org/abs/1608.03983

2. **Cyclical Learning Rates for Training Neural Networks**
   - Smith, WACV 2017
   - https://arxiv.org/abs/1506.01186

---

**æœ€åæ›´æ–°**: 2025-10-09
**ç»“è®º**: Warm Restarts + Dice 2.5æ˜¯æœ€æœ‰æ½œåŠ›çš„ç»„åˆï¼Œé¢„æœŸVal 0.70+
