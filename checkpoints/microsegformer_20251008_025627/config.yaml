augmentation:
  color_jitter:
    brightness: 0.22
    contrast: 0.22
    saturation: 0.12
  horizontal_flip: 0.5
  rotation: 18
  scale_range:
  - 0.88
  - 1.12
data:
  batch_size: 32
  num_workers: 16
  root: data
  val_split: 0.1
experiment:
  description: LMSA with no early stopping - full training for maximum performance
  name: lmsa_ultimate_v1
loss:
  ce_weight: 1.0
  dice_weight: 1.0
  use_class_weights: false
  use_focal: false
model:
  dropout: 0.18
  name: microsegformer
  num_classes: 19
  use_lmsa: true
output_dir: checkpoints/microsegformer_20251008_025627
training:
  early_stopping_patience: null
  epochs: 350
  learning_rate: 8e-4
  max_grad_norm: 1.0
  optimizer: AdamW
  scheduler: CosineAnnealingLR
  use_amp: true
  warmup_epochs: 12
  weight_decay: 0.00015
