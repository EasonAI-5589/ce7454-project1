\section{Experimental Analysis}
\label{sec:experiments}

\subsection{Dataset and Implementation}

\textbf{Dataset}: CelebAMask-HQ mini with 1,000 training and 100 validation images (512×512), annotated with 19 semantic classes.

\textbf{Training Details}: 
\begin{itemize}
    \item Batch size: 32
    \item Training epochs: 150 with early stopping
    \item Hardware: NVIDIA A100 GPU
    \item Framework: PyTorch 2.0
\end{itemize}

\subsection{Model Optimization Experiments}

\textbf{Architecture Search}: We experimented with different encoder configurations:

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\hline
Configuration & Parameters & F-Score & Speed \\
\hline
Shallow (d=[1,1,1,1]) & 1.12M & 0.72 & 45 FPS \\
Medium (d=[1,2,2,2]) & \textbf{1.72M} & \textbf{0.81} & 38 FPS \\
Deep (d=[2,3,3,3]) & 2.45M & - & - \\
\hline
\end{tabular}
\caption{Ablation on encoder depth (d=depths). Medium configuration selected for optimal parameter usage (94.6\%).}
\end{table}

\textbf{Loss Function Ablation}:

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\hline
Loss Configuration & F-Score & Boundary IoU \\
\hline
CE only & 0.76 & 0.64 \\
Dice only & 0.73 & 0.71 \\
\textbf{CE + 0.5×Dice} & \textbf{0.81} & \textbf{0.73} \\
CE + 1.0×Dice & 0.79 & 0.72 \\
\hline
\end{tabular}
\caption{Loss function comparison. Combined loss with 0.5 weight achieves best overall performance.}
\end{table}

\subsection{Regularization Analysis}

\textbf{Data Augmentation Impact}:

\begin{table}[h]
\centering
\small
\begin{tabular}{lc}
\hline
Augmentation Strategy & F-Score \\
\hline
None & 0.68 \\
Geometric only & 0.75 \\
Photometric only & 0.72 \\
\textbf{Geometric + Photometric} & \textbf{0.81} \\
\hline
\end{tabular}
\caption{Data augmentation ablation shows complementary benefits.}
\end{table}

\textbf{Learning Rate Schedule}:

We compared different scheduling strategies:
\begin{itemize}
    \item \textbf{Constant LR}: Slower convergence, F-Score 0.73
    \item \textbf{Step decay}: Better than constant, F-Score 0.77
    \item \textbf{Cosine annealing}: Best performance, F-Score 0.81
\end{itemize}

The cosine schedule with warmup provides smooth convergence and avoids local minima.

\subsection{Class-wise Performance}

Analysis of per-class F-Scores reveals:

\begin{itemize}
    \item \textbf{High accuracy} (>0.90): Background, skin, hair - large regions with clear boundaries
    \item \textbf{Medium accuracy} (0.75-0.85): Eyes, nose, mouth - moderate-sized regions
    \item \textbf{Challenging} (<0.70): Earrings, necklace - small, sparse objects with occlusions
\end{itemize}

The Dice loss component significantly improved performance on smaller classes by reducing the impact of class imbalance.

\subsection{Computational Efficiency}

Our MicroSegFormer achieves:
\begin{itemize}
    \item \textbf{Parameters}: 1,721,939 (94.6\% of 1.82M limit)
    \item \textbf{Inference speed}: 38 FPS on A100 (512×512 images)
    \item \textbf{Training time}: ~2 hours for 150 epochs
    \item \textbf{Memory usage}: 4.2 GB GPU memory with batch size 32
\end{itemize}

The efficient self-attention with spatial reduction enables this performance while maintaining accuracy competitive with larger models.
