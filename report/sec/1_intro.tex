\section{Introduction}
\label{sec:intro}

Face parsing, the task of pixel-level semantic segmentation of facial images into different regions (e.g., eyes, nose, mouth, hair, skin), is crucial for numerous computer vision applications including face recognition, makeup transfer, and face editing. Despite significant advances in semantic segmentation using deep learning, face parsing remains challenging due to the need for fine-grained boundary detection and the large variation in facial appearance, pose, and occlusions.

Recent transformer-based architectures~\cite{xie2021segformer} have shown promising results for semantic segmentation tasks. However, these models often contain millions of parameters, making them computationally expensive and unsuitable for resource-constrained scenarios. This work addresses the challenge of developing an efficient face parsing model that achieves competitive performance while adhering to strict parameter constraints.

\subsection{Problem Statement}

The task is to perform pixel-wise semantic segmentation on the CelebAMask-HQ dataset, which contains facial images annotated with 19 different semantic classes including background, skin, nose, eyes, eyebrows, ears, mouth, lips, hair, hat, earrings, necklace, neck, and clothing. The primary challenges include:

\begin{itemize}
    \item \textbf{Parameter Efficiency}: The model must contain fewer than 1,821,085 trainable parameters
    \item \textbf{Fine-grained Segmentation}: Accurate boundary delineation between adjacent facial regions
    \item \textbf{Class Imbalance}: Significant variation in the spatial extent of different facial components
    \item \textbf{Limited Training Data}: Only 1,000 training images with 100 validation samples
\end{itemize}

\subsection{Our Approach}

We propose MicroSegFormer enhanced with a novel \textbf{Lightweight Multi-Scale Attention (LMSA)} module. Our key contributions include:

\begin{itemize}
    \item \textbf{LMSA Module}: A parameter-efficient attention mechanism with parallel multi-scale convolutions (3×3, 5×5, 7×7) and SE-style channel attention, adding only 1.5\% parameters (+25,984) while improving F-Score by +0.98\%
    \item Efficient hierarchical transformer encoder with spatial reduction attention to minimize computational cost
    \item Lightweight MLP decoder for multi-scale feature aggregation
    \item Simple yet effective loss function (Cross-Entropy + Dice, 1:1 ratio) that outperforms complex alternatives
    \item Comprehensive data augmentation strategy for strong generalization from limited training data
\end{itemize}

\textbf{Key Finding}: Through systematic experiments, we demonstrate that \textit{architectural improvements are more effective than specialized loss functions} for handling class imbalance. Our LMSA module's dynamic attention mechanism implicitly addresses class imbalance, making Focal Loss redundant and even harmful (performance decreased by 1.7-2.3\%).

Our final model achieves 1,747,923 parameters (96.0\% utilization), 0.6819 validation F-Score, and \textbf{0.72 test F-Score}, with test performance exceeding validation by +5.6\%, demonstrating exceptional generalization.
