\begin{abstract}
Face parsing is a fundamental task in computer vision that aims to segment facial regions into semantic categories. This report presents a lightweight transformer-based approach for face parsing on the CelebAMask-HQ dataset. We propose MicroSegFormer, an efficient architecture inspired by SegFormer that achieves strong performance with only 1.72M parameters (94.6\% of the 1.82M limit). Our model employs hierarchical transformer encoders with efficient self-attention mechanisms and a lightweight MLP decoder for multi-scale feature fusion. Through careful optimization of data augmentation, loss functions, and training strategies, we demonstrate competitive face parsing results while maintaining strict parameter constraints. The implementation includes comprehensive experiments on model architecture, regularization techniques, and augmentation strategies.
\end{abstract}