# Ultra-low LR Fine-tuning from 0.6889 checkpoint
# Goal: Carefully fine-tune without breaking the converged model

experiment:
  name: lmsa_finetune_ultralow
  description: "Ultra-low LR fine-tuning from best checkpoint (0.6889) - conservative approach"

model:
  name: microsegformer
  num_classes: 19
  use_lmsa: true
  dropout: 0.1

data:
  root: data
  batch_size: 32
  num_workers: 8
  val_split: 0.1

augmentation:
  horizontal_flip: 0.5
  rotation: 15
  scale_range: [0.9, 1.1]
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.1

loss:
  ce_weight: 1.0
  dice_weight: 1.5  # Keep same as best model
  use_focal: false
  use_class_weights: false

training:
  optimizer: AdamW
  learning_rate: 5e-6  # ⬇️ 160x lower than original 8e-4
  weight_decay: 2e-4  # Slightly higher for regularization
  scheduler: CosineAnnealingLR
  warmup_epochs: 0  # No warmup for fine-tuning
  epochs: 80
  early_stopping_patience: 15  # More aggressive early stop
  max_grad_norm: 0.5  # Lower gradient clipping for stability
  use_amp: true

# This config is meant to be used with continue_training.py
# checkpoint: checkpoints/microsegformer_20251008_025917/best_model.pth
