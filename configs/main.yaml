# Configuration for MicroSegFormer (main model)

model:
  name: microsegformer
  num_classes: 19

data:
  root: data
  batch_size: 32      # A100优化: 避免upsample错误
  num_workers: 16     # 增加数据加载速度
  val_split: 0.1

training:
  epochs: 150
  optimizer: AdamW
  learning_rate: 1.5e-3  # batch32对应的lr
  weight_decay: 5e-4     # 增加正则化: 1e-4 -> 5e-4
  scheduler: CosineAnnealingLR
  warmup_epochs: 5
  max_grad_norm: 1.0
  early_stopping_patience: 30  # 增加patience: 20 -> 30
  use_amp: true          # 开启混合精度训练

loss:
  ce_weight: 1.0
  dice_weight: 0.5

augmentation:
  horizontal_flip: 0.5
  rotation: 15
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.1
  scale_range: [0.9, 1.1]

experiment:
  name: baseline_microsegformer
  description: "Main Transformer model with 1.72M parameters (94.6% usage)"
