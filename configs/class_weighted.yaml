# Class Weighted Configuration - Targeting 0.75 F-Score
# Based on best training (0.6753) + class weights for imbalanced data
# Addresses class imbalance: 7143:1 ratio (hair vs neck_l)

experiment:
  name: "class_weighted_v1"
  description: "MicroSegFormer with class weights - targeting 0.72+ F-Score (breakthrough to 0.75)"

model:
  name: "microsegformer"
  num_classes: 19
  dropout: 0.15  # Proven effective (from 0.6753 training)

data:
  root: "data"
  batch_size: 32
  num_workers: 16
  val_split: 0.1

training:
  epochs: 200
  optimizer: "AdamW"
  learning_rate: 8e-4  # Proven effective
  weight_decay: 1e-4
  scheduler: "CosineAnnealingLR"
  warmup_epochs: 5
  max_grad_norm: 1.0
  early_stopping_patience: 50
  use_amp: true

loss:
  ce_weight: 1.0
  dice_weight: 1.0              # Proven effective (+3.2% improvement)
  use_class_weights: true       # NEW: Main improvement target
  use_focal: false              # Start with class weights only
  focal_alpha: 0.25
  focal_gamma: 2.0

augmentation:
  horizontal_flip: 0.5
  rotation: 15
  scale_range: [0.9, 1.1]      # Matched to working config
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.1

# Improvement Analysis:
# Baseline (no weights):    0.6753 F-Score
# + Class weights:          +0.05-0.07 (expected)
# Target:                   0.72-0.75 F-Score
#
# Class weights handle 7143:1 imbalance:
# - Small classes (eyes, ears): 100-500x higher weight
# - Forces model to learn all classes equally
# - Trade-off: Accuracy may drop 2-3%, but F-Score improves
