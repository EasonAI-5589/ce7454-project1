# Dice 1.5 + Cosine Warm Restarts (LRä¼˜åŒ–ç‰ˆæœ¬)
# Goal: è§£å†³LRè¡°å‡è¿‡å¿«é—®é¢˜ï¼Œé€šè¿‡å‘¨æœŸæ€§é‡å¯æ¢ç´¢æ›´å¥½çš„è§£

experiment:
  name: lmsa_dice1.5_warm_restart
  description: "Dice 1.5 baseline with warm restarts - fix LR decay issue"

model:
  name: microsegformer
  num_classes: 19
  use_lmsa: true
  dropout: 0.15  # ä¿æŒä¸æœ€ä½³æ¨¡å‹ä¸€è‡´

data:
  root: data
  batch_size: 32
  num_workers: 8
  val_split: 0.1

augmentation:
  horizontal_flip: 0.5
  rotation: 15
  scale_range: [0.9, 1.1]
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.1

loss:
  ce_weight: 1.0
  dice_weight: 1.5  # ä¿æŒä¸æœ€ä½³æ¨¡å‹ä¸€è‡´
  use_focal: false
  use_class_weights: false

training:
  optimizer: AdamW
  learning_rate: 8e-4  # ä¸æœ€ä½³æ¨¡å‹ç›¸åŒ
  weight_decay: 1e-4
  scheduler: CosineAnnealingWarmRestarts  # ğŸ”¥ å…³é”®æ”¹åŠ¨
  scheduler_params:
    T_0: 25          # ç¬¬ä¸€æ¬¡é‡å¯åœ¨25è½®ï¼ˆå¯¹åº”åŸæ¥Epoch 92é™„è¿‘ï¼‰
    T_mult: 2        # ä¹‹åå‘¨æœŸç¿»å€ï¼š25, 50, 100
    eta_min: 1e-6  # æœ€ä½LR (1e-6)
  warmup_epochs: 5
  epochs: 200
  early_stopping_patience: 100
  max_grad_norm: 1.0
  use_amp: true
