# Dice 2.5 + æœ€ä¼˜LRç­–ç•¥ç»„åˆ (ç»ˆæä¼˜åŒ–ç‰ˆ)
# Goal: Dice 2.5 + Warm Restarts + å¼ºæ­£åˆ™åŒ– - å†²å‡»0.70+

experiment:
  name: lmsa_dice2.5_lr_optimized
  description: "Dice 2.5 with optimal LR strategy - aiming for 0.70+"

model:
  name: microsegformer
  num_classes: 19
  use_lmsa: true
  dropout: 0.2  # æ›´å¼ºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ

data:
  root: data
  batch_size: 32
  num_workers: 8
  val_split: 0.1

augmentation:
  horizontal_flip: 0.5
  rotation: 15
  scale_range: [0.9, 1.1]
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.1

loss:
  ce_weight: 1.0
  dice_weight: 2.5  # ğŸ”¥ æ¿€è¿›ä¼˜åŒ–å°ç›®æ ‡
  use_focal: false
  use_class_weights: false

training:
  optimizer: AdamW
  learning_rate: 8e-4  # ä¿æŒç¨³å®šçš„åˆå§‹LR
  weight_decay: 2e-4   # ğŸ”¥ æ›´å¼ºweight decay
  scheduler: CosineAnnealingWarmRestarts  # ğŸ”¥ å‘¨æœŸæ€§é‡å¯
  scheduler_params:
    T_0: 30          # ç¬¬ä¸€æ¬¡é‡å¯30è½®
    T_mult: 2        # å‘¨æœŸç¿»å€
    eta_min: 1e-6
  warmup_epochs: 8   # ğŸ”¥ æ›´é•¿warmupç»™Dice 2.5é€‚åº”æ—¶é—´
  epochs: 200
  early_stopping_patience: 100
  max_grad_norm: 1.0
  use_amp: true

# ç»„åˆç­–ç•¥ï¼š
# 1. Dice 2.5: æœ€æ¿€è¿›çš„å°ç›®æ ‡ä¼˜åŒ–
# 2. Warm Restarts: è§£å†³LRè¡°å‡è¿‡å¿«ï¼Œå¤šæ¬¡æ”¶æ•›æœºä¼š
# 3. å¼ºæ­£åˆ™åŒ–: dropout 0.2 + wd 2e-4é˜²æ­¢è¿‡æ‹Ÿåˆ
# 4. é•¿warmup: ç»™å¤æ‚æŸå¤±å‡½æ•°æ›´å¤šçƒ­èº«æ—¶é—´
# é¢„æœŸï¼šVal 0.700-0.710, Test 0.74-0.75
