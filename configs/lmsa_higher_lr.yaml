# LMSA v2 - Higher Learning Rate with Extended Warmup
# 策略: 更高学习率+更长warmup，探索是否能找到更好的优化路径
# 目标: 突破当前局部最优

experiment:
  name: "lmsa_higher_lr"
  description: "Best config (0.6819) with higher LR (1e-3) and extended warmup"

model:
  name: "microsegformer"
  num_classes: 19
  use_lmsa: true
  dropout: 0.15        # ✅ 保持最佳

data:
  root: "data"
  batch_size: 32
  num_workers: 16
  val_split: 0.1

training:
  epochs: 350          # ↑ 增加训练时间
  optimizer: "AdamW"
  learning_rate: 1e-3  # ↑ 从8e-4增加到1e-3
  weight_decay: 1e-4   # ✅ 保持最佳
  scheduler: "CosineAnnealingLR"
  warmup_epochs: 10    # ↑ 从5增加到10，适配更高学习率
  max_grad_norm: 1.0
  early_stopping_patience: null  # 移除early stopping
  use_amp: true

loss:
  ce_weight: 1.0
  dice_weight: 1.0
  use_class_weights: false
  use_focal: false

augmentation:
  horizontal_flip: 0.5
  rotation: 15
  scale_range: [0.9, 1.1]
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.1

# 策略: 更高学习率可能找到更好的最优点
# 更长warmup确保稳定训练
# 预期: Val 0.68-0.70, Test 0.72-0.74
